{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KvfLVdCgm2D",
        "outputId": "77c0fc74-e20e-46db-a0b0-6b6f43598946"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHndvuCogQx3"
      },
      "source": [
        "## AI 허브 한국어 대화(의류) 데이터 활용 예제: Seq2Seq 구현 및 학습\n",
        "- AI 허브 한국어 대화(의류) 데이터셋을 이용한 문장 생성"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4PZI0QHg6Ni",
        "outputId": "8b5c745a-ec82-4595-9bdb-a3275fc0c223"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 11.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 21.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RBCW3aEegQx8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qMKZ2WFgQx9"
      },
      "source": [
        "## 하이퍼 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qZ0BPq2HgQx9"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 200\n",
        "NUM_WORDS = 10000 #Size of the vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU-CVKp4gQx-"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CK8ZKEgUgQx-"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) #원-핫 인코딩 형태로 입력됨\n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_state=True) #return_state=True 해주어어야 h(hidden state), c(cell state) 얻을 수 있음\n",
        "        # GRU: cell state가 없으므로 hidden state값만 추출\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        x = self.emb(x)\n",
        "        _, h, c = self.lstm(x)\n",
        "        return h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGoTEVfkgQx_"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zUwGOGyhgQx_"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True) # return_sequences=True: 출력된 모든 단어를 알아야하기 때문에\n",
        "        self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        x, h, c = inputs\n",
        "        x = self.emb(x)\n",
        "        x, h, c = self.lstm(x, initial_state=[h, c])\n",
        "        return self.dense(x), h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpUENCzFgQyB"
      },
      "source": [
        "## Seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5kWXWoHTgQyB"
      },
      "outputs": [],
      "source": [
        "class Seq2seq(tf.keras.Model): # Encoder + Decoder\n",
        "    def __init__(self, sos, eos):\n",
        "        super(Seq2seq, self).__init__()\n",
        "        self.enc = Encoder()\n",
        "        self.dec = Decoder()\n",
        "        self.sos = sos # start of sequence\n",
        "        self.eos = eos # end of sequence\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        # 학습\n",
        "        if training is True:\n",
        "            x, y = inputs\n",
        "            h, c = self.enc(x)\n",
        "            y, _, _ = self.dec((y, h, c))\n",
        "            return y\n",
        "\n",
        "        # 테스트\n",
        "        else: \n",
        "            x = inputs\n",
        "            h, c = self.enc(x)\n",
        "            y = tf.convert_to_tensor(self.sos) # 첫번째 입력으로 `sos`\n",
        "            y = tf.reshape(y, (1, 1))\n",
        "\n",
        "            seq = tf.TensorArray(tf.int32, 64) # 64길이\n",
        "\n",
        "            # 마지막으로 얻은 출력을 다시 입력으로 넣어주기 위해 for문을 사용\n",
        "            for idx in tf.range(64): \n",
        "                y, h, c = self.dec([y, h, c]) # 이때 y값은 softmax를 거친 값으로 one-hot 벡터로 바꾸고 spare 표현으로 바꾸기 위해 argmax 취해줌 \n",
        "                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
        "                y = tf.reshape(y, (1, 1)) # 배치를 표현하기 위해 (1,1)으로 reshape\n",
        "                seq = seq.write(idx, y)\n",
        "\n",
        "                if y == self.eos:\n",
        "                    break\n",
        "\n",
        "            return tf.reshape(seq.stack(), (1, 64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWFS1kQGgQyC"
      },
      "source": [
        "## 학습, 테스트 루프 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HLn1DvtDgQyC"
      },
      "outputs": [],
      "source": [
        "# Implement training loop\n",
        "@tf.function\n",
        "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
        "    output_labels = labels[:, 1:]\n",
        "    shifted_labels = labels[:, :-1] # 학습할 때 Decoder의 입력으로 들어갈 y\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model([inputs, shifted_labels], training=True)\n",
        "        loss = loss_object(output_labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(output_labels, predictions)\n",
        "\n",
        "# Implement algorithm test\n",
        "@tf.function\n",
        "def test_step(model, inputs):\n",
        "    return model(inputs, training=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMfVGk4VgQyD"
      },
      "source": [
        "## 데이터셋 준비"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/NLP practice/data/aihub_한국어대화_의류.csv')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "XUj2am0JhLuA",
        "outputId": "0229c839-b13a-4c15-8ff7-941583c4c1c8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-81bb9ec3-8c64-4205-bdd5-ebd6db0eae55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SPEAKER</th>\n",
              "      <th>SENTENCE</th>\n",
              "      <th>DOMAINID</th>\n",
              "      <th>DOMAIN</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>SPEAKERID</th>\n",
              "      <th>SENTENCEID</th>\n",
              "      <th>MAIN</th>\n",
              "      <th>SUB</th>\n",
              "      <th>QA</th>\n",
              "      <th>QACNCT</th>\n",
              "      <th>MQ</th>\n",
              "      <th>SQ</th>\n",
              "      <th>UA</th>\n",
              "      <th>SA</th>\n",
              "      <th>개체명</th>\n",
              "      <th>용어사전</th>\n",
              "      <th>지식베이스</th>\n",
              "      <th>Unnamed: 18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>고객</td>\n",
              "      <td>신발은 여기 있는 게 다예요?</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>신발</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>종류별신발제품문의요청</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "      <td>NaN</td>\n",
              "      <td>신발은 여기 있는 게 다예요?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>신발, 여기</td>\n",
              "      <td>NaN</td>\n",
              "      <td>신발/제품</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>점원</td>\n",
              "      <td>네 성인이나 아동 다 있어요</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>신발</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>종류별신발제품문의요청</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>네 성인이나 아동 다 있어요</td>\n",
              "      <td>성인, 아동</td>\n",
              "      <td>NaN</td>\n",
              "      <td>성인/대상, 아동/대상</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>점원</td>\n",
              "      <td>발 사이즈 몇 신으세요?</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>신발</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>종류별신발제품문의요청</td>\n",
              "      <td>사이즈</td>\n",
              "      <td>Q</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>발 사이즈 몇 신으세요?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>발, 사이즈</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>고객</td>\n",
              "      <td>230이요</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>신발</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>종류별신발제품문의요청</td>\n",
              "      <td>사이즈</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>230이요</td>\n",
              "      <td>NaN</td>\n",
              "      <td>230</td>\n",
              "      <td>NaN</td>\n",
              "      <td>230/사이즈</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>점원</td>\n",
              "      <td>편하게 신을 수 있는 거 찾으세요?</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>신발</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>착화감</td>\n",
              "      <td>제품문의</td>\n",
              "      <td>Q</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>편하게 신을 수 있는 거 찾으세요?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81bb9ec3-8c64-4205-bdd5-ebd6db0eae55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81bb9ec3-8c64-4205-bdd5-ebd6db0eae55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81bb9ec3-8c64-4205-bdd5-ebd6db0eae55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "  SPEAKER             SENTENCE DOMAINID  ... 용어사전         지식베이스  Unnamed: 18\n",
              "0      고객     신발은 여기 있는 게 다예요?        B  ...  NaN         신발/제품          0.0\n",
              "1      점원      네 성인이나 아동 다 있어요        B  ...  NaN  성인/대상, 아동/대상          NaN\n",
              "2      점원        발 사이즈 몇 신으세요?        B  ...  NaN           NaN          NaN\n",
              "3      고객                230이요        B  ...  NaN       230/사이즈          NaN\n",
              "4      점원  편하게 신을 수 있는 거 찾으세요?        B  ...  NaN           NaN          NaN\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-A로 이루어진 데이셋으로 정제\n",
        "QA_idx = []\n",
        "for idx in range(len(data)):\n",
        "    if idx != len(data)-1:\n",
        "        if data.loc[idx]['QA'] == 'Q' and data.loc[idx+1]['QA'] == 'A':\n",
        "            QA_idx.append(idx)\n",
        "            QA_idx.append(idx+1)\n",
        "            idx = idx + 1\n"
      ],
      "metadata": {
        "id": "u8xsYDSAnvg0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.loc[QA_idx]\n",
        "data.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "bfqATgbEoLeX",
        "outputId": "dc067385-5253-4614-8a81-3dc95286b6bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-29b98e18-e60b-4184-94ce-2c92d423e489\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SPEAKER</th>\n",
              "      <th>SENTENCE</th>\n",
              "      <th>DOMAINID</th>\n",
              "      <th>DOMAIN</th>\n",
              "      <th>CATEGORY</th>\n",
              "      <th>SPEAKERID</th>\n",
              "      <th>SENTENCEID</th>\n",
              "      <th>MAIN</th>\n",
              "      <th>SUB</th>\n",
              "      <th>QA</th>\n",
              "      <th>QACNCT</th>\n",
              "      <th>MQ</th>\n",
              "      <th>SQ</th>\n",
              "      <th>UA</th>\n",
              "      <th>SA</th>\n",
              "      <th>개체명</th>\n",
              "      <th>용어사전</th>\n",
              "      <th>지식베이스</th>\n",
              "      <th>Unnamed: 18</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14895</th>\n",
              "      <td>점원</td>\n",
              "      <td>그 옷은 십이만 팔천 원이에요</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>의류</td>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>가격문의</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>그 옷은 십이만 팔천 원이에요</td>\n",
              "      <td>옷,십이만 팔천 원</td>\n",
              "      <td>NaN</td>\n",
              "      <td>십이만 팔천 원/가격</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14896</th>\n",
              "      <td>고객</td>\n",
              "      <td>지금 봄옷은 이르지 않나요?</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>의류</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>계절상품문의</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "      <td>NaN</td>\n",
              "      <td>지금 봄옷은 이르지 않나요?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>봄옷</td>\n",
              "      <td>NaN</td>\n",
              "      <td>봄옷/의류</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14897</th>\n",
              "      <td>점원</td>\n",
              "      <td>지금 봄옷 입기 괜찮아요</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>의류</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>계절상품문의</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>지금 봄옷 입기 괜찮아요</td>\n",
              "      <td>봄옷</td>\n",
              "      <td>NaN</td>\n",
              "      <td>봄옷/의류</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14898</th>\n",
              "      <td>고객</td>\n",
              "      <td>요즘 유행하는 색깔이 뭐예요?</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>의류</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>인기제품문의</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "      <td>NaN</td>\n",
              "      <td>요즘 유행하는 색깔이 뭐예요?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>유행,색깔</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14899</th>\n",
              "      <td>점원</td>\n",
              "      <td>요즘 파스텔 톤이 유행이에요</td>\n",
              "      <td>B</td>\n",
              "      <td>의복의류점</td>\n",
              "      <td>의류</td>\n",
              "      <td>0</td>\n",
              "      <td>26</td>\n",
              "      <td>인기제품문의</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>요즘 파스텔 톤이 유행이에요</td>\n",
              "      <td>파스텔 톤,유행</td>\n",
              "      <td>NaN</td>\n",
              "      <td>파스텔 톤/색</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29b98e18-e60b-4184-94ce-2c92d423e489')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-29b98e18-e60b-4184-94ce-2c92d423e489 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-29b98e18-e60b-4184-94ce-2c92d423e489');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      SPEAKER          SENTENCE DOMAINID  ... 용어사전        지식베이스  Unnamed: 18\n",
              "14895      점원  그 옷은 십이만 팔천 원이에요        B  ...  NaN  십이만 팔천 원/가격          NaN\n",
              "14896      고객   지금 봄옷은 이르지 않나요?        B  ...  NaN        봄옷/의류          NaN\n",
              "14897      점원     지금 봄옷 입기 괜찮아요        B  ...  NaN        봄옷/의류          NaN\n",
              "14898      고객  요즘 유행하는 색깔이 뭐예요?        B  ...  NaN          NaN          NaN\n",
              "14899      점원   요즘 파스텔 톤이 유행이에요        B  ...  NaN      파스텔 톤/색          NaN\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "djz5_yvfgQyD"
      },
      "outputs": [],
      "source": [
        "okt = Okt()\n",
        "\n",
        "QA_data = data['SENTENCE']\n",
        "seq = [' '.join(okt.morphs(line))+' \\n' for line in QA_data]\n",
        "\n",
        "questions = seq[::2] \n",
        "answers = ['\\t ' + lines for lines in seq[1::2]] # '\\t': sos로 사용\n",
        "\n",
        "num_sample = len(questions)\n",
        "\n",
        "perm = list(range(num_sample)) # 데이터가 편향된 상태기때문에 섞음\n",
        "random.seed(0)\n",
        "random.shuffle(perm)\n",
        "\n",
        "train_q = list()\n",
        "train_a = list()\n",
        "test_q = list()\n",
        "test_a = list()\n",
        "\n",
        "for idx, qna in enumerate(zip(questions, answers)):\n",
        "    q, a = qna\n",
        "    if perm[idx] > num_sample//5: # 데이터의 4/5\n",
        "        train_q.append(q)\n",
        "        train_a.append(a)\n",
        "    else: # 데이터의 1/5\n",
        "        test_q.append(q)\n",
        "        test_a.append(a)\n",
        "\n",
        "# tokenizer: 각 단어를 숫자로 변형\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS, # filters에 해당하는 문자는 제거 ('\\n'과 '\\t'는 제외)\n",
        "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "\n",
        "tokenizer.fit_on_texts(train_q + train_a)\n",
        "\n",
        "# 숫자로 된 단어의 나열\n",
        "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
        "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
        "\n",
        "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
        "test_a_seq = tokenizer.texts_to_sequences(test_a)\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='pre',\n",
        "                                                        maxlen=64)\n",
        "y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='post', #출력 데이터를 앞쪽으로 두기위해 뒤에 패딩\n",
        "                                                        maxlen=65) #'\\t'와 '\\n'가 붙어있는 상황이라 앞에 하나 떼고 사용하고, 뒤에 하나 떼고 사용 => 실제 길이는 64로 사용\n",
        "\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n",
        "                                                       value=0,\n",
        "                                                       padding='pre',\n",
        "                                                       maxlen=64)\n",
        "y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=65)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8capPFDwgQyF"
      },
      "source": [
        "## 학습 환경 정의\n",
        "### 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7Sw9hwPEgQyF"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = Seq2seq(sos=tokenizer.word_index['\\t'],\n",
        "                eos=tokenizer.word_index['\\n'])\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Define performance metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xfoSDPlgQyG"
      },
      "source": [
        "## 학습 루프 동작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e43db610dcf04a50826778d6def1bfae",
            "61761defb897442f8a7979daeef2172e",
            "48f3a6a5e4ab47a1958fba7cc7b6df41",
            "041ece6dd758459089120d9d2c9ab8d4",
            "880c5f81b73045c09dcf8755ed1d3b08",
            "547181f53a154b6589bd7e3867082239",
            "3bb50ba2d0e743b2956aa8ea24a40b75",
            "f8c7b9fb1ebb482184a14c62736acdbd",
            "10628409eadc44c2b1fbde57c83d0b8b",
            "dc66ba98a2424e8aa5c1e4550e5b3060",
            "c8242b5621db43818af3c893c57f4c06"
          ]
        },
        "id": "EKbLHK3ggQyG",
        "outputId": "6ce3e5af-b2ca-4397-ab35-ee1c9795df60"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e43db610dcf04a50826778d6def1bfae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 1.0648168325424194, Accuracy: 89.67798614501953\n",
            "Epoch 2, Loss: 0.5948834419250488, Accuracy: 91.4739990234375\n",
            "Epoch 3, Loss: 0.5452613830566406, Accuracy: 91.96234893798828\n",
            "Epoch 4, Loss: 0.5253956317901611, Accuracy: 92.08804321289062\n",
            "Epoch 5, Loss: 0.5078579783439636, Accuracy: 92.2298812866211\n",
            "Epoch 6, Loss: 0.49200260639190674, Accuracy: 92.38900756835938\n",
            "Epoch 7, Loss: 0.4774836301803589, Accuracy: 92.57408905029297\n",
            "Epoch 8, Loss: 0.4604460299015045, Accuracy: 92.73754119873047\n",
            "Epoch 9, Loss: 0.4439573884010315, Accuracy: 92.90331268310547\n",
            "Epoch 10, Loss: 0.4258338510990143, Accuracy: 93.0243911743164\n",
            "Epoch 11, Loss: 0.4089261293411255, Accuracy: 93.18266296386719\n",
            "Epoch 12, Loss: 0.39179450273513794, Accuracy: 93.29825592041016\n",
            "Epoch 13, Loss: 0.37579813599586487, Accuracy: 93.40982055664062\n",
            "Epoch 14, Loss: 0.3595995008945465, Accuracy: 93.515625\n",
            "Epoch 15, Loss: 0.3432786762714386, Accuracy: 93.64420318603516\n",
            "Epoch 16, Loss: 0.32711470127105713, Accuracy: 93.7860336303711\n",
            "Epoch 17, Loss: 0.31169942021369934, Accuracy: 93.96967315673828\n",
            "Epoch 18, Loss: 0.29683801531791687, Accuracy: 94.17349243164062\n",
            "Epoch 19, Loss: 0.2823704481124878, Accuracy: 94.38624572753906\n",
            "Epoch 20, Loss: 0.26786938309669495, Accuracy: 94.61543273925781\n",
            "Epoch 21, Loss: 0.2539413571357727, Accuracy: 94.83049011230469\n",
            "Epoch 22, Loss: 0.2413424253463745, Accuracy: 95.0599594116211\n",
            "Epoch 23, Loss: 0.2288876324892044, Accuracy: 95.26522064208984\n",
            "Epoch 24, Loss: 0.21740157902240753, Accuracy: 95.46673583984375\n",
            "Epoch 25, Loss: 0.20628675818443298, Accuracy: 95.67803955078125\n",
            "Epoch 26, Loss: 0.19542613625526428, Accuracy: 95.86744689941406\n",
            "Epoch 27, Loss: 0.18535034358501434, Accuracy: 96.08049011230469\n",
            "Epoch 28, Loss: 0.17549531161785126, Accuracy: 96.28113555908203\n",
            "Epoch 29, Loss: 0.16609175503253937, Accuracy: 96.43738555908203\n",
            "Epoch 30, Loss: 0.15714462101459503, Accuracy: 96.61064147949219\n",
            "Epoch 31, Loss: 0.14844931662082672, Accuracy: 96.8014907836914\n",
            "Epoch 32, Loss: 0.1401379257440567, Accuracy: 96.98800659179688\n",
            "Epoch 33, Loss: 0.13233545422554016, Accuracy: 97.16558837890625\n",
            "Epoch 34, Loss: 0.12465617060661316, Accuracy: 97.31636047363281\n",
            "Epoch 35, Loss: 0.11686449497938156, Accuracy: 97.49077606201172\n",
            "Epoch 36, Loss: 0.10963994264602661, Accuracy: 97.63636016845703\n",
            "Epoch 37, Loss: 0.10289514064788818, Accuracy: 97.80298614501953\n",
            "Epoch 38, Loss: 0.09605197608470917, Accuracy: 97.95404815673828\n",
            "Epoch 39, Loss: 0.08968255668878555, Accuracy: 98.11634826660156\n",
            "Epoch 40, Loss: 0.08377079665660858, Accuracy: 98.24118041992188\n",
            "Epoch 41, Loss: 0.078060582280159, Accuracy: 98.37638092041016\n",
            "Epoch 42, Loss: 0.07272946089506149, Accuracy: 98.5006332397461\n",
            "Epoch 43, Loss: 0.06745541840791702, Accuracy: 98.60671997070312\n",
            "Epoch 44, Loss: 0.06265072524547577, Accuracy: 98.70069885253906\n",
            "Epoch 45, Loss: 0.05777877941727638, Accuracy: 98.81976318359375\n",
            "Epoch 46, Loss: 0.053465962409973145, Accuracy: 98.90394592285156\n",
            "Epoch 47, Loss: 0.049244750291109085, Accuracy: 99.00397491455078\n",
            "Epoch 48, Loss: 0.045380041003227234, Accuracy: 99.08584594726562\n",
            "Epoch 49, Loss: 0.04177920147776604, Accuracy: 99.16830444335938\n",
            "Epoch 50, Loss: 0.038200076669454575, Accuracy: 99.25103759765625\n",
            "Epoch 51, Loss: 0.03491656854748726, Accuracy: 99.31272888183594\n",
            "Epoch 52, Loss: 0.031862854957580566, Accuracy: 99.39315795898438\n",
            "Epoch 53, Loss: 0.0293600894510746, Accuracy: 99.44793701171875\n",
            "Epoch 54, Loss: 0.026746734976768494, Accuracy: 99.49982452392578\n",
            "Epoch 55, Loss: 0.02422100491821766, Accuracy: 99.55546569824219\n",
            "Epoch 56, Loss: 0.02225016988813877, Accuracy: 99.59092712402344\n",
            "Epoch 57, Loss: 0.020180165767669678, Accuracy: 99.63301086425781\n",
            "Epoch 58, Loss: 0.018594911321997643, Accuracy: 99.66645050048828\n",
            "Epoch 59, Loss: 0.016922026872634888, Accuracy: 99.69903564453125\n",
            "Epoch 60, Loss: 0.015710197389125824, Accuracy: 99.71344757080078\n",
            "Epoch 61, Loss: 0.014227073639631271, Accuracy: 99.73910522460938\n",
            "Epoch 62, Loss: 0.013124286197125912, Accuracy: 99.75899505615234\n",
            "Epoch 63, Loss: 0.012138976715505123, Accuracy: 99.77484893798828\n",
            "Epoch 64, Loss: 0.011287537403404713, Accuracy: 99.78638458251953\n",
            "Epoch 65, Loss: 0.01051515992730856, Accuracy: 99.7973403930664\n",
            "Epoch 66, Loss: 0.009807609021663666, Accuracy: 99.80799865722656\n",
            "Epoch 67, Loss: 0.009362831711769104, Accuracy: 99.80655670166016\n",
            "Epoch 68, Loss: 0.008854559622704983, Accuracy: 99.81578826904297\n",
            "Epoch 69, Loss: 0.008331459015607834, Accuracy: 99.82212829589844\n",
            "Epoch 70, Loss: 0.007960404269397259, Accuracy: 99.8302001953125\n",
            "Epoch 71, Loss: 0.007723984774202108, Accuracy: 99.82501220703125\n",
            "Epoch 72, Loss: 0.0072817010805010796, Accuracy: 99.83423614501953\n",
            "Epoch 73, Loss: 0.006892923731356859, Accuracy: 99.83222198486328\n",
            "Epoch 74, Loss: 0.0065610590390861034, Accuracy: 99.83654022216797\n",
            "Epoch 75, Loss: 0.0064003425650298595, Accuracy: 99.84375\n",
            "Epoch 76, Loss: 0.007646229118108749, Accuracy: 99.81117248535156\n",
            "Epoch 77, Loss: 0.02427275851368904, Accuracy: 99.37096405029297\n",
            "Epoch 78, Loss: 0.013204752467572689, Accuracy: 99.70537567138672\n",
            "Epoch 79, Loss: 0.007461614441126585, Accuracy: 99.8302001953125\n",
            "Epoch 80, Loss: 0.005754214245826006, Accuracy: 99.84893798828125\n",
            "Epoch 81, Loss: 0.0054044234566390514, Accuracy: 99.85066986083984\n",
            "Epoch 82, Loss: 0.005141659174114466, Accuracy: 99.84922790527344\n",
            "Epoch 83, Loss: 0.005086399614810944, Accuracy: 99.85297393798828\n",
            "Epoch 84, Loss: 0.004982538055628538, Accuracy: 99.84922790527344\n",
            "Epoch 85, Loss: 0.0048043690621852875, Accuracy: 99.85326385498047\n",
            "Epoch 86, Loss: 0.004917724523693323, Accuracy: 99.84922790527344\n",
            "Epoch 87, Loss: 0.004792042076587677, Accuracy: 99.84980010986328\n",
            "Epoch 88, Loss: 0.004774034023284912, Accuracy: 99.85441589355469\n",
            "Epoch 89, Loss: 0.0047937617637217045, Accuracy: 99.85153198242188\n",
            "Epoch 90, Loss: 0.004702170379459858, Accuracy: 99.85211181640625\n",
            "Epoch 91, Loss: 0.004587797448039055, Accuracy: 99.85700988769531\n",
            "Epoch 92, Loss: 0.0046693673357367516, Accuracy: 99.85355377197266\n",
            "Epoch 93, Loss: 0.0047251321375370026, Accuracy: 99.84807586669922\n",
            "Epoch 94, Loss: 0.004532296676188707, Accuracy: 99.85441589355469\n",
            "Epoch 95, Loss: 0.004690145142376423, Accuracy: 99.85009765625\n",
            "Epoch 96, Loss: 0.004773263353854418, Accuracy: 99.84864807128906\n",
            "Epoch 97, Loss: 0.004973154515028, Accuracy: 99.84346008300781\n",
            "Epoch 98, Loss: 0.004835240542888641, Accuracy: 99.84951782226562\n",
            "Epoch 99, Loss: 0.0046547781676054, Accuracy: 99.85326385498047\n",
            "Epoch 100, Loss: 0.004772666841745377, Accuracy: 99.84778594970703\n",
            "Epoch 101, Loss: 0.011782695539295673, Accuracy: 99.66761016845703\n",
            "Epoch 102, Loss: 0.013566608540713787, Accuracy: 99.62753295898438\n",
            "Epoch 103, Loss: 0.006738241761922836, Accuracy: 99.8180923461914\n",
            "Epoch 104, Loss: 0.0045383949764072895, Accuracy: 99.85153198242188\n",
            "Epoch 105, Loss: 0.004074708092957735, Accuracy: 99.85816192626953\n",
            "Epoch 106, Loss: 0.003992618527263403, Accuracy: 99.85643768310547\n",
            "Epoch 107, Loss: 0.003859509015455842, Accuracy: 99.85441589355469\n",
            "Epoch 108, Loss: 0.0038436991162598133, Accuracy: 99.85441589355469\n",
            "Epoch 109, Loss: 0.003840178716927767, Accuracy: 99.85355377197266\n",
            "Epoch 110, Loss: 0.003837329102680087, Accuracy: 99.85470581054688\n",
            "Epoch 111, Loss: 0.003905750112608075, Accuracy: 99.85211181640625\n",
            "Epoch 112, Loss: 0.003858928568661213, Accuracy: 99.85470581054688\n",
            "Epoch 113, Loss: 0.0038530866149812937, Accuracy: 99.85845184326172\n",
            "Epoch 114, Loss: 0.003845898900181055, Accuracy: 99.85845184326172\n",
            "Epoch 115, Loss: 0.0038794882129877806, Accuracy: 99.85614776611328\n",
            "Epoch 116, Loss: 0.003969839308410883, Accuracy: 99.85499572753906\n",
            "Epoch 117, Loss: 0.004091036971658468, Accuracy: 99.84835815429688\n",
            "Epoch 118, Loss: 0.004103463608771563, Accuracy: 99.8541259765625\n",
            "Epoch 119, Loss: 0.004012803081423044, Accuracy: 99.85124206542969\n",
            "Epoch 120, Loss: 0.004231623839586973, Accuracy: 99.84922790527344\n",
            "Epoch 121, Loss: 0.004524142947047949, Accuracy: 99.84259796142578\n",
            "Epoch 122, Loss: 0.004642109852284193, Accuracy: 99.83914184570312\n",
            "Epoch 123, Loss: 0.004294931888580322, Accuracy: 99.84893798828125\n",
            "Epoch 124, Loss: 0.004164545796811581, Accuracy: 99.85326385498047\n",
            "Epoch 125, Loss: 0.003906790167093277, Accuracy: 99.85557556152344\n",
            "Epoch 126, Loss: 0.003818807192146778, Accuracy: 99.85211181640625\n",
            "Epoch 127, Loss: 0.003778813872486353, Accuracy: 99.85153198242188\n",
            "Epoch 128, Loss: 0.003654822474345565, Accuracy: 99.85816192626953\n",
            "Epoch 129, Loss: 0.0036387545987963676, Accuracy: 99.85527801513672\n",
            "Epoch 130, Loss: 0.003645275253802538, Accuracy: 99.8590316772461\n",
            "Epoch 131, Loss: 0.0036931459326297045, Accuracy: 99.85211181640625\n",
            "Epoch 132, Loss: 0.0037883531767874956, Accuracy: 99.85527801513672\n",
            "Epoch 133, Loss: 0.0036767167039215565, Accuracy: 99.85787963867188\n",
            "Epoch 134, Loss: 0.005446417722851038, Accuracy: 99.8180923461914\n",
            "Epoch 135, Loss: 0.01878325268626213, Accuracy: 99.43266296386719\n",
            "Epoch 136, Loss: 0.0077363839372992516, Accuracy: 99.76879119873047\n",
            "Epoch 137, Loss: 0.004190946463495493, Accuracy: 99.85355377197266\n",
            "Epoch 138, Loss: 0.003606849117204547, Accuracy: 99.8572998046875\n",
            "Epoch 139, Loss: 0.0034281564876437187, Accuracy: 99.8590316772461\n",
            "Epoch 140, Loss: 0.003434963757172227, Accuracy: 99.85671997070312\n",
            "Epoch 141, Loss: 0.0034004582557827234, Accuracy: 99.8558578491211\n",
            "Epoch 142, Loss: 0.0033931967336684465, Accuracy: 99.8572998046875\n",
            "Epoch 143, Loss: 0.0034065968357026577, Accuracy: 99.85960388183594\n",
            "Epoch 144, Loss: 0.003418500302359462, Accuracy: 99.85614776611328\n",
            "Epoch 145, Loss: 0.0034209690056741238, Accuracy: 99.85931396484375\n",
            "Epoch 146, Loss: 0.003371042665094137, Accuracy: 99.8590316772461\n",
            "Epoch 147, Loss: 0.003426097333431244, Accuracy: 99.86017608642578\n",
            "Epoch 148, Loss: 0.0034502397757023573, Accuracy: 99.85499572753906\n",
            "Epoch 149, Loss: 0.0033536157570779324, Accuracy: 99.86017608642578\n",
            "Epoch 150, Loss: 0.0033903904259204865, Accuracy: 99.85499572753906\n",
            "Epoch 151, Loss: 0.003380413167178631, Accuracy: 99.8587417602539\n",
            "Epoch 152, Loss: 0.003398184897378087, Accuracy: 99.85845184326172\n",
            "Epoch 153, Loss: 0.0034119561314582825, Accuracy: 99.85931396484375\n",
            "Epoch 154, Loss: 0.0034135524183511734, Accuracy: 99.85671997070312\n",
            "Epoch 155, Loss: 0.0034232810139656067, Accuracy: 99.86133575439453\n",
            "Epoch 156, Loss: 0.003531171241775155, Accuracy: 99.85527801513672\n",
            "Epoch 157, Loss: 0.00558652076870203, Accuracy: 99.80799865722656\n",
            "Epoch 158, Loss: 0.01650579273700714, Accuracy: 99.48426055908203\n",
            "Epoch 159, Loss: 0.006495101843029261, Accuracy: 99.79070281982422\n",
            "Epoch 160, Loss: 0.004119018092751503, Accuracy: 99.85153198242188\n",
            "Epoch 161, Loss: 0.003497179364785552, Accuracy: 99.85499572753906\n",
            "Epoch 162, Loss: 0.0033941357396543026, Accuracy: 99.8590316772461\n",
            "Epoch 163, Loss: 0.0033160920720547438, Accuracy: 99.86075592041016\n",
            "Epoch 164, Loss: 0.0033272558357566595, Accuracy: 99.8572998046875\n",
            "Epoch 165, Loss: 0.003295691916719079, Accuracy: 99.85614776611328\n",
            "Epoch 166, Loss: 0.003306758124381304, Accuracy: 99.85527801513672\n",
            "Epoch 167, Loss: 0.0033175612334161997, Accuracy: 99.85557556152344\n",
            "Epoch 168, Loss: 0.0032487642019987106, Accuracy: 99.85787963867188\n",
            "Epoch 169, Loss: 0.003266718005761504, Accuracy: 99.86219787597656\n",
            "Epoch 170, Loss: 0.003305445658043027, Accuracy: 99.85787963867188\n",
            "Epoch 171, Loss: 0.003307154169306159, Accuracy: 99.8558578491211\n",
            "Epoch 172, Loss: 0.0032745704520493746, Accuracy: 99.86075592041016\n",
            "Epoch 173, Loss: 0.0032815870363265276, Accuracy: 99.85989379882812\n",
            "Epoch 174, Loss: 0.003312458982691169, Accuracy: 99.85614776611328\n",
            "Epoch 175, Loss: 0.00329973129555583, Accuracy: 99.85671997070312\n",
            "Epoch 176, Loss: 0.0032554364297538996, Accuracy: 99.85960388183594\n",
            "Epoch 177, Loss: 0.0032782824710011482, Accuracy: 99.85787963867188\n",
            "Epoch 178, Loss: 0.0033545487094670534, Accuracy: 99.85557556152344\n",
            "Epoch 179, Loss: 0.0034198095090687275, Accuracy: 99.8558578491211\n",
            "Epoch 180, Loss: 0.003506764303892851, Accuracy: 99.85527801513672\n",
            "Epoch 181, Loss: 0.0055884444154798985, Accuracy: 99.80713653564453\n",
            "Epoch 182, Loss: 0.012549580074846745, Accuracy: 99.59669494628906\n",
            "Epoch 183, Loss: 0.006361169740557671, Accuracy: 99.79618072509766\n",
            "Epoch 184, Loss: 0.003907316830009222, Accuracy: 99.85037994384766\n",
            "Epoch 185, Loss: 0.003398737870156765, Accuracy: 99.85614776611328\n",
            "Epoch 186, Loss: 0.0032380251213908195, Accuracy: 99.85499572753906\n",
            "Epoch 187, Loss: 0.0032418002374470234, Accuracy: 99.85989379882812\n",
            "Epoch 188, Loss: 0.003241762286052108, Accuracy: 99.85499572753906\n",
            "Epoch 189, Loss: 0.0032470650039613247, Accuracy: 99.8619155883789\n",
            "Epoch 190, Loss: 0.003238538047298789, Accuracy: 99.8558578491211\n",
            "Epoch 191, Loss: 0.0032165509182959795, Accuracy: 99.85700988769531\n",
            "Epoch 192, Loss: 0.0031987871043384075, Accuracy: 99.85700988769531\n",
            "Epoch 193, Loss: 0.0032324111089110374, Accuracy: 99.85787963867188\n",
            "Epoch 194, Loss: 0.003244462888687849, Accuracy: 99.85960388183594\n",
            "Epoch 195, Loss: 0.0031823606695979834, Accuracy: 99.86105346679688\n",
            "Epoch 196, Loss: 0.0032489474397152662, Accuracy: 99.85671997070312\n",
            "Epoch 197, Loss: 0.0032738239970058203, Accuracy: 99.85643768310547\n",
            "Epoch 198, Loss: 0.003225987544283271, Accuracy: 99.85960388183594\n",
            "Epoch 199, Loss: 0.0032201444264501333, Accuracy: 99.85816192626953\n",
            "Epoch 200, Loss: 0.0032713364344090223, Accuracy: 99.85499572753906\n"
          ]
        }
      ],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    for seqs, labels in train_ds:\n",
        "        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
        "\n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
        "    print(template.format(epoch + 1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result() * 100))\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJAOFNQWgQyG"
      },
      "source": [
        "## 테스트 루프"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiBI74DcgQyG",
        "outputId": "a6a73dfb-5b4c-4364-c8e9-3e2ae2eb1cb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_\n",
            "q:  ['편하게 신 을 수 있는 거 찾으세요 \\n']\n",
            "a:  ['\\t 네 봄 이니까 편하게 신 을 수 있는 거 \\n']\n",
            "p:  ['혼자 천천히 둘러볼게요 \\n']\n",
            "_\n",
            "q:  ['굽 좀 높은 거 없나요 \\n']\n",
            "a:  ['\\t 봄 상품 은 아직 어른 제품 이 많이 안 나왔습니다 \\n']\n",
            "p:  ['3 번 피팅룸 이 비어있으니 들어가세요 \\n']\n",
            "_\n",
            "q:  ['또 안 들어와요 \\n']\n",
            "a:  ['\\t 네 이건 다 끝났어요 \\n']\n",
            "p:  ['다음 주 에 들어와요 \\n']\n",
            "_\n",
            "q:  ['이 거 는 천이 죠 \\n']\n",
            "a:  ['\\t 네 맞아요 \\n']\n",
            "p:  ['네 공단 천 원 이에요 \\n']\n",
            "_\n",
            "q:  ['며칠 까지 휴무 예요 \\n']\n",
            "a:  ['\\t 설 까지 쉬 고 다음 날 열 거 같아요 \\n']\n",
            "p:  ['9시 까지 해 요 \\n']\n",
            "_\n",
            "q:  ['이 거 예요 다 돌 인가요 \\n']\n",
            "a:  ['\\t 도 있고 도자기 도 있어요 \\n']\n",
            "p:  ['네 이 쪽 은 입학생 들용 이건 초등 아니고 좀 뜨기 때문 에 불편한 점 이 있습니다 \\n']\n",
            "_\n",
            "q:  ['몇 시 에 문 닫아요 \\n']\n",
            "a:  ['\\t 8시 까지 합니다 \\n']\n",
            "p:  ['9시 50분 이요 \\n']\n",
            "_\n",
            "q:  ['원단 은 뭐 예요 \\n']\n",
            "a:  ['\\t 원단 이 구김 안 가고 참 괜찮아요 \\n']\n",
            "p:  ['그거 입생로랑 휴대폰 케이스 인데 가죽 이에요 \\n']\n",
            "_\n",
            "q:  ['브로치 같은 건 어디 있나요 \\n']\n",
            "a:  ['\\t 여기 있는 거 밖에 없어요 \\n']\n",
            "p:  ['그레이 는 지금 재고 가 다 나갔어요 \\n']\n",
            "_\n",
            "q:  ['온누리 상품권 도 되죠 \\n']\n",
            "a:  ['\\t 네 됩니다 \\n']\n",
            "p:  ['네 생활 방수 다 돼요 \\n']\n",
            "_\n",
            "q:  ['안 에 하면 안 떨어져요 \\n']\n",
            "a:  ['\\t 안 떨어져요 \\n']\n",
            "p:  ['네 이건 국산 이에요 \\n']\n",
            "_\n",
            "q:  ['얼마 예요 \\n']\n",
            "a:  ['\\t 핀 거 는 좀 싼 거고 이 거 는 3만 원대 예요 \\n']\n",
            "p:  ['정가 는 10만 9천 원 이예 요 \\n']\n",
            "_\n",
            "q:  ['이 거 는 목걸이 링 이에요 \\n']\n",
            "a:  ['\\t 아뇨 반지 예요 \\n']\n",
            "p:  ['네 들어 보시 면 알겠지만 굉장히 가벼운 편이 에요 \\n']\n",
            "_\n",
            "q:  ['금 이에요 \\n']\n",
            "a:  ['\\t 액세서리 예요 금 아니예요 \\n']\n",
            "p:  ['아니요 여기 있는 것 들 은 저렴한 거 라 가죽 이 아니에요 \\n']\n",
            "_\n",
            "q:  ['이 거 한 번 해봐도 돼요 \\n']\n",
            "a:  ['\\t 네 가능합니다 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['가격 은 어떻게 돼요 \\n']\n",
            "a:  ['\\t 모양 을 보고 을 내 봐야 해 요 \\n']\n",
            "p:  ['그거 는 삼만 원 이고 여기 앵 클 들은 다 사만 원 이에요 \\n']\n",
            "_\n",
            "q:  ['인 팔찌 가 있어요 \\n']\n",
            "a:  ['\\t 여기 있습니다 \\n']\n",
            "p:  ['그거 는 확실히 몰라요 \\n']\n",
            "_\n",
            "q:  ['에 있는 금 상 하고 여기 랑 가격 차이 가 많이 나 요 \\n']\n",
            "a:  ['\\t 그 쪽 은 가 많이 아무래도 여기 가 더 \\n']\n",
            "p:  ['없어요 \\n']\n",
            "_\n",
            "q:  ['악세서리 어떤 거 찾으세요 \\n']\n",
            "a:  ['\\t 스카프 같은 거 \\n']\n",
            "p:  ['애가 뚱뚱해요 \\n']\n",
            "_\n",
            "q:  ['이건 얼마 예요 \\n']\n",
            "a:  ['\\t 13000원 입니다 \\n']\n",
            "p:  ['4만 5천 원 이요 \\n']\n",
            "_\n",
            "q:  ['어른 들 꺼는 없어요 \\n']\n",
            "a:  ['\\t 다 빠졌어요 \\n']\n",
            "p:  ['제 가 하려고 해 요 \\n']\n",
            "_\n",
            "q:  ['팔찌 좀 핫 한 제품 뭐 가 있어요 \\n']\n",
            "a:  ['\\t 깔끔하게 이런 거 잘나가요 \\n']\n",
            "p:  ['이 거 다 슬립 온 \\n']\n",
            "_\n",
            "q:  ['이 게 순금 인지 18 케이 인지 한 번 봐주시겠어요 \\n']\n",
            "a:  ['\\t 18 케이 입니다 \\n']\n",
            "p:  ['그거 는 5만 8천 원 짜 리 예요 \\n']\n",
            "_\n",
            "q:  ['에서 구매 한 반지 여기 서 세척 할 수 있나요 \\n']\n",
            "a:  ['\\t 공장 에 야합니다 \\n']\n",
            "p:  ['이 쪽 으로 오셔서 마음 에 드시는 스타일 골라 보세요 \\n']\n",
            "_\n",
            "q:  ['착용 해볼 수 있어요 \\n']\n",
            "a:  ['\\t 비닐 마시고 그냥 착용 해보세요 \\n']\n",
            "p:  ['네 가능하세요 \\n']\n",
            "_\n",
            "q:  ['원 로 보여 드릴 까요 \\n']\n",
            "a:  ['\\t 원 로 좀 붙는 걸 로 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['재질 은 뭔 데 요 \\n']\n",
            "a:  ['\\t \\n']\n",
            "p:  ['가죽 은 아닌데 괜찮은 재질 이 예요 \\n']\n",
            "_\n",
            "q:  ['이건 팔찌 예요 \\n']\n",
            "a:  ['\\t 네 맞아요 \\n']\n",
            "p:  ['네 맞아요 \\n']\n",
            "_\n",
            "q:  ['카카오 페이 결제 되나요 \\n']\n",
            "a:  ['\\t 계좌 이체 되기 때문 에 카카오 페이 계좌 하시면 돼요 \\n']\n",
            "p:  ['3만 원 이 요 \\n']\n",
            "_\n",
            "q:  ['현금 으로 하면 할인 해주시나요 \\n']\n",
            "a:  ['\\t 만 원 인데 천 원 할인 해드릴게요 \\n']\n",
            "p:  ['아니요 카드 랑 가격 똑같아요 \\n']\n",
            "_\n",
            "q:  ['여기 있는 건 다 은침 인가요 \\n']\n",
            "a:  ['\\t 은침 이 아니면 이 쪽 은 다 금 이 예요 \\n']\n",
            "p:  ['네 니트 입니다 \\n']\n",
            "_\n",
            "q:  ['이건 얼마 예요 \\n']\n",
            "a:  ['\\t 이요 \\n']\n",
            "p:  ['4만 5천 원 이요 \\n']\n",
            "_\n",
            "q:  ['착용 해볼 수 있어요 \\n']\n",
            "a:  ['\\t 착용 은 안되고 살짝 됩니다 \\n']\n",
            "p:  ['네 가능하세요 \\n']\n",
            "_\n",
            "q:  ['이 거 두 개 가 디자인 은 똑같은 거 예요 \\n']\n",
            "a:  ['\\t 디자인 은 똑같은데 구슬 있고 차이 에요 \\n']\n",
            "p:  ['겨울 거 는 디자인 이 요 \\n']\n",
            "_\n",
            "q:  ['저녁 에 몇 시 에 문 \\n']\n",
            "a:  ['\\t 6시 반 이요 \\n']\n",
            "p:  ['한 번 신어 보세요 \\n']\n",
            "_\n",
            "q:  ['이런 거 는 사이즈 가 \\n']\n",
            "a:  ['\\t 프리 사이즈 입니다 \\n']\n",
            "p:  ['네 그거 는 아니에요 \\n']\n",
            "_\n",
            "q:  ['코트 는 얼마나 해 요 \\n']\n",
            "a:  ['\\t 98000원 입니다 \\n']\n",
            "p:  ['5만 천 원 입니다 \\n']\n",
            "_\n",
            "q:  ['나가면 또 들어오나요 \\n']\n",
            "a:  ['\\t 들어올 겁니다 \\n']\n",
            "p:  ['티셔츠 사이즈 도 있습니다 \\n']\n",
            "_\n",
            "q:  ['\\n']\n",
            "a:  ['\\t 네 중국 옷 은 없을 거 예요 \\n']\n",
            "p:  ['지금 보신 거 는 안됩니다 \\n']\n",
            "_\n",
            "q:  ['남방 같은 거 어떠세요 \\n']\n",
            "a:  ['\\t 점잖아 보인다 \\n']\n",
            "p:  ['아 좋을 거 같은데요 \\n']\n",
            "_\n",
            "q:  ['이런 거 는 얼마 예요 \\n']\n",
            "a:  ['\\t 그거 는 45000원 이에요 \\n']\n",
            "p:  ['이만 원 이요 작은 거 는 만 오천 원 이 고요 \\n']\n",
            "_\n",
            "q:  ['이런 거 가격 좀 잘 해주세요 \\n']\n",
            "a:  ['\\t 싸게 \\n']\n",
            "p:  ['그 가격 은 목걸이 랑 한 세트 로 30만원 이에요 \\n']\n",
            "_\n",
            "q:  ['이런 흰 면 바지 는 얼마 예요 \\n']\n",
            "a:  ['\\t 이요 \\n']\n",
            "p:  ['이 거 는 28000원 입니다 \\n']\n",
            "_\n",
            "q:  ['여기 옷 은 브랜드 는 아니죠 \\n']\n",
            "a:  ['\\t 서울 에서 물건 이 들어와요 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['이 거 는 겨울 거 죠 \\n']\n",
            "a:  ['\\t 네 맞아요 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['이 거 는 세탁 은 어떻게 해 요 \\n']\n",
            "a:  ['\\t 그냥 손 세탁 하시면 돼요 \\n']\n",
            "p:  ['물 빨래 하셔도 돼요 \\n']\n",
            "_\n",
            "q:  ['그럼 이번 만 카드 로 계산 해드릴게요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['그럼요 \\n']\n",
            "_\n",
            "q:  ['이런 종류 어떤 게 잘 나가요 \\n']\n",
            "a:  ['\\t 이런 거 는 봄 에 검은색 바지 와 같이 많이 입어요 \\n']\n",
            "p:  ['이 베이지색 이 가장 잘 나가요 \\n']\n",
            "_\n",
            "q:  ['현금 으로 사면 또 싸게 돼요 \\n']\n",
            "a:  ['\\t 현금 하시면 천 원 정도 더 빼어 드릴 수 있어요 \\n']\n",
            "p:  ['전부 다 현금 가라 할인 된 가격 은 없어요 \\n']\n",
            "_\n",
            "q:  ['어떤 스타일 찾으세요 \\n']\n",
            "a:  ['\\t 아기자기한 거 좋아하거든요 \\n']\n",
            "p:  ['봄 에 신 을 발 편한 로퍼 찾아요 \\n']\n",
            "_\n",
            "q:  ['저 거 색상 은 이 거 한 개 뿐 이에요 \\n']\n",
            "a:  ['\\t 저 디자인 하나 밖에 안 남았어요 \\n']\n",
            "p:  ['네 \\n']\n"
          ]
        }
      ],
      "source": [
        "for idx, (test_seq, test_labels) in enumerate(test_ds):\n",
        "    if idx > 50:\n",
        "        break\n",
        "    prediction = test_step(model, test_seq)\n",
        "    test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n",
        "    gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n",
        "    texts = tokenizer.sequences_to_texts(prediction.numpy())\n",
        "    print('_')\n",
        "    print('q: ', test_text)\n",
        "    print('a: ', gt_text)\n",
        "    print('p: ', texts)\n",
        "\n",
        "\n",
        "# 문장에 단어들이 없거나 끊어진 이유는 tokenizer에서 10000개의 word만 사용했기 때문에 이 word에 해당되지 않는 경우 제거되기 때문!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ijicsTZNs6IJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AI 허브 한국어 대화(의류) 데이터 활용 예제: \bAttention 매커니즘 구현 및 학습\n",
        "- Seq2Seq 모델의 정확도 및 성능과 Attention 매커니즘을 활용한 모델과의 성능 차이를 비교해보자"
      ],
      "metadata": {
        "id": "6h_UYpMGbBac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder with Attention Mechanism"
      ],
      "metadata": {
        "id": "2hOfuifQyVb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64) \n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True) \n",
        "        # Attention 매커니즘 적용할 때 key, value는 encoder에서 나오는 hidden state를 전부 활용했었어야 했음\n",
        "        # 그러기 위해 모든 hidden state를 sequences 형태로 출력해주기 위해 returnn_sequences=True 가 추가됨\n",
        "\n",
        "    def call(self, x, training=False, mask=None):\n",
        "        x = self.emb(x)\n",
        "        H, h, c = self.lstm(x)\n",
        "        return H, h, c"
      ],
      "metadata": {
        "id": "WKBMX2HPp3_C"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder with Attention Mechanism"
      ],
      "metadata": {
        "id": "OefkF1-gyXfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.emb = tf.keras.layers.Embedding(NUM_WORDS, 64)\n",
        "        # attention mechanism: LSTM 출력에 Attention value를 concatenate해서 dense layer로 넘겨주는 것\n",
        "        self.lstm = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)\n",
        "        self.att = tf.keras.layers.Attention() # attention mechanism\n",
        "        self.dense = tf.keras.layers.Dense(NUM_WORDS, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        y_, s0, c0, H = inputs # y_: shifted output, s0과 c0: 처음 들어오는 decoder단의 hidden state와 cell state\n",
        "        y_ = self.emb(y_)\n",
        "        # S: 모든 hidden state 받아온 것\n",
        "        S, h, c = self.lstm(y_, initial_state=[s0, c0]) # shifted output과 initial_state를 입력받아 모든 sequence의 출력을 내줌\n",
        "        \n",
        "        # S를 query로 사용, Encoder단에서 받아온 H는 key와 value로 사용\n",
        "        # query로 사용할 때는 S를 그대로 사용하는 것이 아니라, 한 time-step 앞선 것을 사용해야 됨!!\n",
        "        S_ = tf.concat([s0[:, tf.newaxis, :], S[:, :-1, :]], axis=1) \n",
        "        # s0[:, tf.newaxis, :] : 맨 처음에 들어오는 건 길이가 1이라 2차원 형태 -> 이를 3차원으로 확장\n",
        "        # S[:, :-1, :]] : 마지막 hidden state껀 배제하여 총 길이가 64라면 63개만 사용 => 1+63 = 총 64의 길이\n",
        "        A = self.att([S_, H])\n",
        "        # attention value를 dense layer로 넣어주기 전에 S(hidden state)에 A(attention value)를 concatenate\n",
        "        y = tf.concat([S, A], axis=-1)\n",
        "\n",
        "        return self.dense(y), h, c"
      ],
      "metadata": {
        "id": "E-GuIE7_yYn6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq with Attention Mechanism"
      ],
      "metadata": {
        "id": "4SL-8RMilVQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2seq(tf.keras.Model): # Encoder + Decoder\n",
        "    def __init__(self, sos, eos):\n",
        "        super(Seq2seq, self).__init__()\n",
        "        self.enc = Encoder()\n",
        "        self.dec = Decoder()\n",
        "        self.sos = sos # start of sequence\n",
        "        self.eos = eos # end of sequence\n",
        "\n",
        "    def call(self, inputs, training=False, mask=None):\n",
        "        # 학습\n",
        "        if training is True:\n",
        "            x, y = inputs\n",
        "            H, h, c = self.enc(x)\n",
        "            y, _, _ = self.dec((y, h, c, H))\n",
        "            return y\n",
        "\n",
        "        # 테스트\n",
        "        else: \n",
        "            x = inputs\n",
        "            H, h, c = self.enc(x)\n",
        "            y = tf.convert_to_tensor(self.sos) # 첫번째 입력으로 `sos`\n",
        "            y = tf.reshape(y, (1, 1))\n",
        "\n",
        "            seq = tf.TensorArray(tf.int32, 64) # 64길이\n",
        "\n",
        "            # 마지막으로 얻은 출력을 다시 입력으로 넣어주기 위해 for문을 사용\n",
        "            for idx in tf.range(64): \n",
        "                y, h, c = self.dec([y, h, c, H]) # 이때 y값은 softmax를 거친 값으로 one-hot 벡터로 바꾸고 spare 표현으로 바꾸기 위해 argmax 취해줌 \n",
        "                y = tf.cast(tf.argmax(y, axis=-1), dtype=tf.int32)\n",
        "                y = tf.reshape(y, (1, 1)) # 배치를 표현하기 위해 (1,1)으로 reshape\n",
        "                seq = seq.write(idx, y)\n",
        "\n",
        "                if y == self.eos:\n",
        "                    break\n",
        "\n",
        "            return tf.reshape(seq.stack(), (1, 64))"
      ],
      "metadata": {
        "id": "l32RsGU0lPWj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습, 테스트 루프 정의"
      ],
      "metadata": {
        "id": "T_wz6c-9ltv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement training loop\n",
        "@tf.function\n",
        "def train_step(model, inputs, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
        "    output_labels = labels[:, 1:]\n",
        "    shifted_labels = labels[:, :-1] # 학습할 때 Decoder의 입력으로 들어갈 y\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model([inputs, shifted_labels], training=True)\n",
        "        loss = loss_object(output_labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_loss(loss)\n",
        "    train_accuracy(output_labels, predictions)\n",
        "\n",
        "# Implement algorithm test\n",
        "@tf.function\n",
        "def test_step(model, inputs):\n",
        "    return model(inputs, training=False)"
      ],
      "metadata": {
        "id": "hiYti6dKlot7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okt = Okt()\n",
        "\n",
        "QA_data = data['SENTENCE']\n",
        "seq = [' '.join(okt.morphs(line))+' \\n' for line in QA_data]\n",
        "\n",
        "questions = seq[::2] \n",
        "answers = ['\\t ' + lines for lines in seq[1::2]] # '\\t': sos로 사용\n",
        "\n",
        "num_sample = len(questions)\n",
        "\n",
        "perm = list(range(num_sample)) # 데이터가 편향된 상태기때문에 섞음\n",
        "random.seed(0)\n",
        "random.shuffle(perm)\n",
        "\n",
        "train_q = list()\n",
        "train_a = list()\n",
        "test_q = list()\n",
        "test_a = list()\n",
        "\n",
        "for idx, qna in enumerate(zip(questions, answers)):\n",
        "    q, a = qna\n",
        "    if perm[idx] > num_sample//5: # 데이터의 4/5\n",
        "        train_q.append(q)\n",
        "        train_a.append(a)\n",
        "    else: # 데이터의 1/5\n",
        "        test_q.append(q)\n",
        "        test_a.append(a)\n",
        "\n",
        "# tokenizer: 각 단어를 숫자로 변형\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS, # filters에 해당하는 문자는 제거 ('\\n'과 '\\t'는 제외)\n",
        "                                                  filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~')\n",
        "\n",
        "tokenizer.fit_on_texts(train_q + train_a)\n",
        "\n",
        "# 숫자로 된 단어의 나열\n",
        "train_q_seq = tokenizer.texts_to_sequences(train_q)\n",
        "train_a_seq = tokenizer.texts_to_sequences(train_a)\n",
        "\n",
        "test_q_seq = tokenizer.texts_to_sequences(test_q)\n",
        "test_a_seq = tokenizer.texts_to_sequences(test_a)\n",
        "\n",
        "x_train = tf.keras.preprocessing.sequence.pad_sequences(train_q_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='pre',\n",
        "                                                        maxlen=64)\n",
        "y_train = tf.keras.preprocessing.sequence.pad_sequences(train_a_seq,\n",
        "                                                        value=0,\n",
        "                                                        padding='post', #출력 데이터를 앞쪽으로 두기위해 뒤에 패딩\n",
        "                                                        maxlen=65) #'\\t'와 '\\n'가 붙어있는 상황이라 앞에 하나 떼고 사용하고, 뒤에 하나 떼고 사용 => 실제 길이는 64로 사용\n",
        "\n",
        "\n",
        "x_test = tf.keras.preprocessing.sequence.pad_sequences(test_q_seq,\n",
        "                                                       value=0,\n",
        "                                                       padding='pre',\n",
        "                                                       maxlen=64)\n",
        "y_test = tf.keras.preprocessing.sequence.pad_sequences(test_a_seq,\n",
        "                                                       value=0,\n",
        "                                                       padding='post',\n",
        "                                                       maxlen=65)\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32).prefetch(1024)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1).prefetch(1024)"
      ],
      "metadata": {
        "id": "aOYomPeWlvaj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 환경 정의\n",
        "### 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"
      ],
      "metadata": {
        "id": "DI1pLvzLl5rK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model = Seq2seq(sos=tokenizer.word_index['\\t'],\n",
        "                eos=tokenizer.word_index['\\n'])\n",
        "\n",
        "# Define loss and optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Define performance metrics\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
      ],
      "metadata": {
        "id": "EzFYAFEel5zS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 루프\n"
      ],
      "metadata": {
        "id": "Lmpj6Apwl_p9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    for seqs, labels in train_ds:\n",
        "        train_step(model, seqs, labels, loss_object, optimizer, train_loss, train_accuracy)\n",
        "\n",
        "    template = 'Epoch {}, Loss: {}, Accuracy: {}'\n",
        "    print(template.format(epoch + 1,\n",
        "                          train_loss.result(),\n",
        "                          train_accuracy.result() * 100))\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtunuGtYl8HS",
        "outputId": "37bdb796-1704-49bb-d956-afcc18c2f6f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9920271635055542, Accuracy: 89.816650390625\n",
            "Epoch 2, Loss: 0.5803816318511963, Accuracy: 91.8026351928711\n",
            "Epoch 3, Loss: 0.542046070098877, Accuracy: 92.11485290527344\n",
            "Epoch 4, Loss: 0.5167677402496338, Accuracy: 92.28753662109375\n",
            "Epoch 5, Loss: 0.49235090613365173, Accuracy: 92.48097229003906\n",
            "Epoch 6, Loss: 0.46798476576805115, Accuracy: 92.65538787841797\n",
            "Epoch 7, Loss: 0.445870965719223, Accuracy: 92.81855010986328\n",
            "Epoch 8, Loss: 0.4247305393218994, Accuracy: 92.97970581054688\n",
            "Epoch 9, Loss: 0.40266188979148865, Accuracy: 93.14315795898438\n",
            "Epoch 10, Loss: 0.38166549801826477, Accuracy: 93.28845977783203\n",
            "Epoch 11, Loss: 0.360933780670166, Accuracy: 93.43260192871094\n",
            "Epoch 12, Loss: 0.3378351330757141, Accuracy: 93.58366394042969\n",
            "Epoch 13, Loss: 0.31380921602249146, Accuracy: 93.83706665039062\n",
            "Epoch 14, Loss: 0.28822365403175354, Accuracy: 94.16397857666016\n",
            "Epoch 15, Loss: 0.2640449106693268, Accuracy: 94.5358657836914\n",
            "Epoch 16, Loss: 0.23959213495254517, Accuracy: 94.921875\n",
            "Epoch 17, Loss: 0.2165042906999588, Accuracy: 95.3110580444336\n",
            "Epoch 18, Loss: 0.19444985687732697, Accuracy: 95.68380737304688\n",
            "Epoch 19, Loss: 0.1741154044866562, Accuracy: 96.06261444091797\n",
            "Epoch 20, Loss: 0.1551007479429245, Accuracy: 96.44286346435547\n",
            "Epoch 21, Loss: 0.1369291990995407, Accuracy: 96.85741424560547\n",
            "Epoch 22, Loss: 0.12020905315876007, Accuracy: 97.23189544677734\n",
            "Epoch 23, Loss: 0.10530669242143631, Accuracy: 97.5885009765625\n",
            "Epoch 24, Loss: 0.0911468043923378, Accuracy: 97.91887664794922\n",
            "Epoch 25, Loss: 0.07844176888465881, Accuracy: 98.26078796386719\n",
            "Epoch 26, Loss: 0.0666470155119896, Accuracy: 98.5548324584961\n",
            "Epoch 27, Loss: 0.056591808795928955, Accuracy: 98.80851745605469\n",
            "Epoch 28, Loss: 0.048291198909282684, Accuracy: 99.00282287597656\n",
            "Epoch 29, Loss: 0.04052887111902237, Accuracy: 99.21212005615234\n",
            "Epoch 30, Loss: 0.034044016152620316, Accuracy: 99.35222625732422\n",
            "Epoch 31, Loss: 0.0284108966588974, Accuracy: 99.47474670410156\n",
            "Epoch 32, Loss: 0.024275140836834908, Accuracy: 99.56642150878906\n",
            "Epoch 33, Loss: 0.020465755835175514, Accuracy: 99.64281463623047\n",
            "Epoch 34, Loss: 0.017430013045668602, Accuracy: 99.69845581054688\n",
            "Epoch 35, Loss: 0.01505920011550188, Accuracy: 99.74198150634766\n",
            "Epoch 36, Loss: 0.013582049869000912, Accuracy: 99.76129913330078\n",
            "Epoch 37, Loss: 0.012041050009429455, Accuracy: 99.78666687011719\n",
            "Epoch 38, Loss: 0.010723371058702469, Accuracy: 99.80944061279297\n",
            "Epoch 39, Loss: 0.009961174800992012, Accuracy: 99.816650390625\n",
            "Epoch 40, Loss: 0.00947827659547329, Accuracy: 99.816650390625\n",
            "Epoch 41, Loss: 0.008824801072478294, Accuracy: 99.82616424560547\n",
            "Epoch 42, Loss: 0.009548306465148926, Accuracy: 99.81636047363281\n",
            "Epoch 43, Loss: 0.012916943989694118, Accuracy: 99.76505279541016\n",
            "Epoch 44, Loss: 0.016531849279999733, Accuracy: 99.65867614746094\n",
            "Epoch 45, Loss: 0.01420385017991066, Accuracy: 99.71488952636719\n",
            "Epoch 46, Loss: 0.0100850285962224, Accuracy: 99.79013061523438\n",
            "Epoch 47, Loss: 0.008019577711820602, Accuracy: 99.82731628417969\n",
            "Epoch 48, Loss: 0.007155055645853281, Accuracy: 99.83366394042969\n",
            "Epoch 49, Loss: 0.006406501401215792, Accuracy: 99.84375\n",
            "Epoch 50, Loss: 0.006022924557328224, Accuracy: 99.85240173339844\n",
            "Epoch 51, Loss: 0.006032821722328663, Accuracy: 99.84835815429688\n",
            "Epoch 52, Loss: 0.006034800782799721, Accuracy: 99.85182189941406\n",
            "Epoch 53, Loss: 0.005699094384908676, Accuracy: 99.85845184326172\n",
            "Epoch 54, Loss: 0.005778673570603132, Accuracy: 99.84922790527344\n",
            "Epoch 55, Loss: 0.005865130107849836, Accuracy: 99.84893798828125\n",
            "Epoch 56, Loss: 0.005782518535852432, Accuracy: 99.84720611572266\n",
            "Epoch 57, Loss: 0.005858991760760546, Accuracy: 99.8454818725586\n",
            "Epoch 58, Loss: 0.005799923557788134, Accuracy: 99.84749603271484\n",
            "Epoch 59, Loss: 0.005545956548303366, Accuracy: 99.8526840209961\n",
            "Epoch 60, Loss: 0.0055280220694839954, Accuracy: 99.8541259765625\n",
            "Epoch 61, Loss: 0.005332308821380138, Accuracy: 99.85297393798828\n",
            "Epoch 62, Loss: 0.005593888461589813, Accuracy: 99.84432220458984\n",
            "Epoch 63, Loss: 0.006758462637662888, Accuracy: 99.82183837890625\n",
            "Epoch 64, Loss: 0.03378812223672867, Accuracy: 99.05933380126953\n",
            "Epoch 65, Loss: 0.021722592413425446, Accuracy: 99.41535949707031\n",
            "Epoch 66, Loss: 0.008697496727108955, Accuracy: 99.78522491455078\n",
            "Epoch 67, Loss: 0.005746925715357065, Accuracy: 99.84606170654297\n",
            "Epoch 68, Loss: 0.005004010628908873, Accuracy: 99.84807586669922\n",
            "Epoch 69, Loss: 0.004798642825335264, Accuracy: 99.85355377197266\n",
            "Epoch 70, Loss: 0.00467205373570323, Accuracy: 99.85182189941406\n",
            "Epoch 71, Loss: 0.0045809452421963215, Accuracy: 99.85931396484375\n",
            "Epoch 72, Loss: 0.004656468518078327, Accuracy: 99.85470581054688\n",
            "Epoch 73, Loss: 0.004589691758155823, Accuracy: 99.85297393798828\n",
            "Epoch 74, Loss: 0.004634297452867031, Accuracy: 99.85758972167969\n",
            "Epoch 75, Loss: 0.004562597256153822, Accuracy: 99.85671997070312\n",
            "Epoch 76, Loss: 0.004690213594585657, Accuracy: 99.85355377197266\n",
            "Epoch 77, Loss: 0.004681792110204697, Accuracy: 99.85066986083984\n",
            "Epoch 78, Loss: 0.004482511430978775, Accuracy: 99.85643768310547\n",
            "Epoch 79, Loss: 0.004729428794234991, Accuracy: 99.85095977783203\n",
            "Epoch 80, Loss: 0.0047221314162015915, Accuracy: 99.85557556152344\n",
            "Epoch 81, Loss: 0.004690045956522226, Accuracy: 99.85614776611328\n",
            "Epoch 82, Loss: 0.004493469372391701, Accuracy: 99.85700988769531\n",
            "Epoch 83, Loss: 0.004603059962391853, Accuracy: 99.85326385498047\n",
            "Epoch 84, Loss: 0.004534874577075243, Accuracy: 99.85499572753906\n",
            "Epoch 85, Loss: 0.004641411360353231, Accuracy: 99.84951782226562\n",
            "Epoch 86, Loss: 0.004954572767019272, Accuracy: 99.85211181640625\n",
            "Epoch 87, Loss: 0.005359770730137825, Accuracy: 99.83942413330078\n",
            "Epoch 88, Loss: 0.0154248783364892, Accuracy: 99.55776977539062\n",
            "Epoch 89, Loss: 0.020599951967597008, Accuracy: 99.39633178710938\n",
            "Epoch 90, Loss: 0.009798677638173103, Accuracy: 99.72411346435547\n",
            "Epoch 91, Loss: 0.005893461871892214, Accuracy: 99.82731628417969\n",
            "Epoch 92, Loss: 0.004633116535842419, Accuracy: 99.85037994384766\n",
            "Epoch 93, Loss: 0.004329030867666006, Accuracy: 99.85557556152344\n",
            "Epoch 94, Loss: 0.00411995779722929, Accuracy: 99.85931396484375\n",
            "Epoch 95, Loss: 0.004185137338936329, Accuracy: 99.85470581054688\n",
            "Epoch 96, Loss: 0.0040697078220546246, Accuracy: 99.85557556152344\n",
            "Epoch 97, Loss: 0.004154738038778305, Accuracy: 99.85383605957031\n",
            "Epoch 98, Loss: 0.004030425101518631, Accuracy: 99.8587417602539\n",
            "Epoch 99, Loss: 0.004098115023225546, Accuracy: 99.8526840209961\n",
            "Epoch 100, Loss: 0.004055025987327099, Accuracy: 99.85643768310547\n",
            "Epoch 101, Loss: 0.004121572244912386, Accuracy: 99.85557556152344\n",
            "Epoch 102, Loss: 0.0040775020606815815, Accuracy: 99.8572998046875\n",
            "Epoch 103, Loss: 0.004046782851219177, Accuracy: 99.85614776611328\n",
            "Epoch 104, Loss: 0.004118339624255896, Accuracy: 99.8572998046875\n",
            "Epoch 105, Loss: 0.004156310111284256, Accuracy: 99.85499572753906\n",
            "Epoch 106, Loss: 0.004157517571002245, Accuracy: 99.8526840209961\n",
            "Epoch 107, Loss: 0.004094262607395649, Accuracy: 99.85671997070312\n",
            "Epoch 108, Loss: 0.004156630020588636, Accuracy: 99.8526840209961\n",
            "Epoch 109, Loss: 0.004237731918692589, Accuracy: 99.85182189941406\n",
            "Epoch 110, Loss: 0.004080093465745449, Accuracy: 99.8558578491211\n",
            "Epoch 111, Loss: 0.004167499952018261, Accuracy: 99.85960388183594\n",
            "Epoch 112, Loss: 0.004172871354967356, Accuracy: 99.85441589355469\n",
            "Epoch 113, Loss: 0.004634431563317776, Accuracy: 99.8451919555664\n",
            "Epoch 114, Loss: 0.018542928621172905, Accuracy: 99.44620513916016\n",
            "Epoch 115, Loss: 0.015250932425260544, Accuracy: 99.5462417602539\n",
            "Epoch 116, Loss: 0.007005754858255386, Accuracy: 99.79330444335938\n",
            "Epoch 117, Loss: 0.004629621282219887, Accuracy: 99.84720611572266\n",
            "Epoch 118, Loss: 0.004035830032080412, Accuracy: 99.85787963867188\n",
            "Epoch 119, Loss: 0.0038688676431775093, Accuracy: 99.85700988769531\n",
            "Epoch 120, Loss: 0.0038120041135698557, Accuracy: 99.85326385498047\n",
            "Epoch 121, Loss: 0.003720982000231743, Accuracy: 99.8572998046875\n",
            "Epoch 122, Loss: 0.003793541109189391, Accuracy: 99.8541259765625\n",
            "Epoch 123, Loss: 0.003756813006475568, Accuracy: 99.8590316772461\n",
            "Epoch 124, Loss: 0.0037632021121680737, Accuracy: 99.85182189941406\n",
            "Epoch 125, Loss: 0.003811158938333392, Accuracy: 99.85557556152344\n",
            "Epoch 126, Loss: 0.0037843738682568073, Accuracy: 99.85700988769531\n",
            "Epoch 127, Loss: 0.003824851708486676, Accuracy: 99.85643768310547\n",
            "Epoch 128, Loss: 0.003808393841609359, Accuracy: 99.85527801513672\n",
            "Epoch 129, Loss: 0.0037997192703187466, Accuracy: 99.8558578491211\n",
            "Epoch 130, Loss: 0.0037788818590343, Accuracy: 99.85499572753906\n",
            "Epoch 131, Loss: 0.003780112601816654, Accuracy: 99.85787963867188\n",
            "Epoch 132, Loss: 0.003849045606330037, Accuracy: 99.85153198242188\n",
            "Epoch 133, Loss: 0.003762071719393134, Accuracy: 99.85557556152344\n",
            "Epoch 134, Loss: 0.003743333974853158, Accuracy: 99.85845184326172\n",
            "Epoch 135, Loss: 0.003768277820199728, Accuracy: 99.85383605957031\n",
            "Epoch 136, Loss: 0.003906848840415478, Accuracy: 99.85355377197266\n",
            "Epoch 137, Loss: 0.003761607687920332, Accuracy: 99.85527801513672\n",
            "Epoch 138, Loss: 0.0037708322051912546, Accuracy: 99.8558578491211\n",
            "Epoch 139, Loss: 0.003823623526841402, Accuracy: 99.85355377197266\n",
            "Epoch 140, Loss: 0.003794366493821144, Accuracy: 99.8572998046875\n",
            "Epoch 141, Loss: 0.0038346308283507824, Accuracy: 99.85671997070312\n",
            "Epoch 142, Loss: 0.003821945982053876, Accuracy: 99.85700988769531\n",
            "Epoch 143, Loss: 0.004136884585022926, Accuracy: 99.84835815429688\n",
            "Epoch 144, Loss: 0.011888321489095688, Accuracy: 99.63734436035156\n",
            "Epoch 145, Loss: 0.01781611330807209, Accuracy: 99.44793701171875\n",
            "Epoch 146, Loss: 0.008323908783495426, Accuracy: 99.73910522460938\n",
            "Epoch 147, Loss: 0.004788429941982031, Accuracy: 99.83827209472656\n",
            "Epoch 148, Loss: 0.003874952206388116, Accuracy: 99.85614776611328\n",
            "Epoch 149, Loss: 0.003673068480566144, Accuracy: 99.85441589355469\n",
            "Epoch 150, Loss: 0.003509991569444537, Accuracy: 99.85931396484375\n",
            "Epoch 151, Loss: 0.003510302398353815, Accuracy: 99.85758972167969\n",
            "Epoch 152, Loss: 0.003509610891342163, Accuracy: 99.85989379882812\n",
            "Epoch 153, Loss: 0.003475465578958392, Accuracy: 99.85845184326172\n",
            "Epoch 154, Loss: 0.003487749956548214, Accuracy: 99.85557556152344\n",
            "Epoch 155, Loss: 0.0034609949216246605, Accuracy: 99.8604736328125\n",
            "Epoch 156, Loss: 0.0034956196323037148, Accuracy: 99.85614776611328\n",
            "Epoch 157, Loss: 0.00350253120996058, Accuracy: 99.8587417602539\n",
            "Epoch 158, Loss: 0.0035323742777109146, Accuracy: 99.8572998046875\n",
            "Epoch 159, Loss: 0.0035393419675529003, Accuracy: 99.85787963867188\n",
            "Epoch 160, Loss: 0.0034915779251605272, Accuracy: 99.85758972167969\n",
            "Epoch 161, Loss: 0.003565458580851555, Accuracy: 99.8541259765625\n",
            "Epoch 162, Loss: 0.003549993736669421, Accuracy: 99.85499572753906\n",
            "Epoch 163, Loss: 0.0035455054603517056, Accuracy: 99.85643768310547\n",
            "Epoch 164, Loss: 0.0035236794501543045, Accuracy: 99.85671997070312\n",
            "Epoch 165, Loss: 0.0035257753916084766, Accuracy: 99.85643768310547\n",
            "Epoch 166, Loss: 0.0035927905701100826, Accuracy: 99.85845184326172\n",
            "Epoch 167, Loss: 0.003496413351967931, Accuracy: 99.86075592041016\n",
            "Epoch 168, Loss: 0.0035302997566759586, Accuracy: 99.85816192626953\n",
            "Epoch 169, Loss: 0.0034674794878810644, Accuracy: 99.85960388183594\n",
            "Epoch 170, Loss: 0.003561871824786067, Accuracy: 99.85527801513672\n",
            "Epoch 171, Loss: 0.0035020888317376375, Accuracy: 99.86133575439453\n",
            "Epoch 172, Loss: 0.0036040141712874174, Accuracy: 99.85383605957031\n",
            "Epoch 173, Loss: 0.003679111134260893, Accuracy: 99.85614776611328\n",
            "Epoch 174, Loss: 0.0036311871372163296, Accuracy: 99.85182189941406\n",
            "Epoch 175, Loss: 0.0037299387622624636, Accuracy: 99.85614776611328\n",
            "Epoch 176, Loss: 0.005485805217176676, Accuracy: 99.8048324584961\n",
            "Epoch 177, Loss: 0.018330108374357224, Accuracy: 99.42631530761719\n",
            "Epoch 178, Loss: 0.009946632198989391, Accuracy: 99.68634796142578\n",
            "Epoch 179, Loss: 0.005110415630042553, Accuracy: 99.82674407958984\n",
            "Epoch 180, Loss: 0.0037963141221553087, Accuracy: 99.85326385498047\n",
            "Epoch 181, Loss: 0.003489332040771842, Accuracy: 99.8558578491211\n",
            "Epoch 182, Loss: 0.0033727448899298906, Accuracy: 99.8590316772461\n",
            "Epoch 183, Loss: 0.003381958696991205, Accuracy: 99.85787963867188\n",
            "Epoch 184, Loss: 0.0034124620724469423, Accuracy: 99.85527801513672\n",
            "Epoch 185, Loss: 0.003313260618597269, Accuracy: 99.8590316772461\n",
            "Epoch 186, Loss: 0.0033584791235625744, Accuracy: 99.85700988769531\n",
            "Epoch 187, Loss: 0.0032960299868136644, Accuracy: 99.8587417602539\n",
            "Epoch 188, Loss: 0.0032953682821244, Accuracy: 99.8558578491211\n",
            "Epoch 189, Loss: 0.00330848665907979, Accuracy: 99.8572998046875\n",
            "Epoch 190, Loss: 0.0033488909248262644, Accuracy: 99.85557556152344\n",
            "Epoch 191, Loss: 0.00340107548981905, Accuracy: 99.8590316772461\n",
            "Epoch 192, Loss: 0.0033021285198628902, Accuracy: 99.8558578491211\n",
            "Epoch 193, Loss: 0.0033419679384678602, Accuracy: 99.85758972167969\n",
            "Epoch 194, Loss: 0.0033551177475601435, Accuracy: 99.8604736328125\n",
            "Epoch 195, Loss: 0.003393870545551181, Accuracy: 99.85931396484375\n",
            "Epoch 196, Loss: 0.0033991269301623106, Accuracy: 99.85643768310547\n",
            "Epoch 197, Loss: 0.003375004045665264, Accuracy: 99.85700988769531\n",
            "Epoch 198, Loss: 0.0033629569225013256, Accuracy: 99.85758972167969\n",
            "Epoch 199, Loss: 0.003413494909182191, Accuracy: 99.85787963867188\n",
            "Epoch 200, Loss: 0.0033500888384878635, Accuracy: 99.8572998046875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트 루프"
      ],
      "metadata": {
        "id": "gnHhUNUAmD7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9XxRuOQr6xA",
        "outputId": "17550663-0e9a-4fa3-aefd-c6cb270f8963"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1356"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, (test_seq, test_labels) in enumerate(test_ds):\n",
        "    if idx > 200 and idx < 250:\n",
        "        prediction = test_step(model, test_seq)\n",
        "        test_text = tokenizer.sequences_to_texts(test_seq.numpy())\n",
        "        gt_text = tokenizer.sequences_to_texts(test_labels.numpy())\n",
        "        texts = tokenizer.sequences_to_texts(prediction.numpy())\n",
        "        print('_')\n",
        "        print('q: ', test_text)\n",
        "        print('a: ', gt_text)\n",
        "        print('p: ', texts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3L5caSGmETK",
        "outputId": "9339aeb8-b794-4435-d370-7de309385df5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_\n",
            "q:  ['이 거 는 얼 만 데 요 \\n']\n",
            "a:  ['\\t 만 천 원 입니다 \\n']\n",
            "p:  ['이 거 처음 에 나왔을 때 십이만 원 받은 건데 팔만 원 이요 \\n']\n",
            "_\n",
            "q:  ['엄마 들 입 을 건데 요 좀 밝은 색 으로 요 \\n']\n",
            "a:  ['\\t 겨울 옷 이라서 이 정도 가 밝은 편이 에요 \\n']\n",
            "p:  ['저 게 면 이 게 잘 나가요 \\n']\n",
            "_\n",
            "q:  ['바꾸러 오면 좀 그렇겠죠 \\n']\n",
            "a:  ['\\t 교환 은 됩니다 \\n']\n",
            "p:  ['네 들 한테 나와서 안 에 얇게 고 추천 해 요 \\n']\n",
            "_\n",
            "q:  ['이 거 드라이 해야 해 요 \\n']\n",
            "a:  ['\\t 드라이 하면 좋지만 물 세탁 도 돼요 \\n']\n",
            "p:  ['울 샴푸 로 물 에 몇 방울 떨어뜨려서 조물조물 하면 됩니다 \\n']\n",
            "_\n",
            "q:  ['색깔 은 뭐 있어요 \\n']\n",
            "a:  ['\\t 밤색 있어요 \\n']\n",
            "p:  ['회색 한 장 있어요 \\n']\n",
            "_\n",
            "q:  ['얼마 예요 \\n']\n",
            "a:  ['\\t 구천 원 이에요 \\n']\n",
            "p:  ['17만 9천 원 이에요 \\n']\n",
            "_\n",
            "q:  ['이 코트 는 얼마 예요 \\n']\n",
            "a:  ['\\t 이 거 는 사만 원 에 드리겠습니다 \\n']\n",
            "p:  ['그 제품 도 삼만 원 이에요 \\n']\n",
            "_\n",
            "q:  ['나이 가 어떻게 되시는데요 \\n']\n",
            "a:  ['\\t 팔십 둘 이 요 \\n']\n",
            "p:  ['스물 여덟 인데 \\n']\n",
            "_\n",
            "q:  ['그 몇 문 까지 신 을 수 있어요 \\n']\n",
            "a:  ['\\t 이백육십 까지 신 을 수 있어요 \\n']\n",
            "p:  ['지금 110 까지 영업 나와요 \\n']\n",
            "_\n",
            "q:  ['수면 양말 이건 얼마 예요 \\n']\n",
            "a:  ['\\t 오천 원 이에요 \\n']\n",
            "p:  ['3만 원 짜 리도 있고 2만 원 짜 리도 있고 다양해요 \\n']\n",
            "_\n",
            "q:  ['어른 들 는 얼마 정도 에요 \\n']\n",
            "a:  ['\\t 만 오천 원 부터 비싼 거 는 사만 오천 원 까지 있어요 \\n']\n",
            "p:  ['이건 50 퍼센트 인데 할인 해서 40 고 이 게 7만 원 이에요 \\n']\n",
            "_\n",
            "q:  ['사이즈 는 있나요 \\n']\n",
            "a:  ['\\t 105 호 는 이 거 보온 용 하나 남아있어요 \\n']\n",
            "p:  ['지금 시즌 오프라 없는 것 도 있어요 \\n']\n",
            "_\n",
            "q:  ['이 거 얼마 예요 \\n']\n",
            "a:  ['\\t 이만 육천 원 이에요 \\n']\n",
            "p:  ['14만 9천 원 이요 \\n']\n",
            "_\n",
            "q:  ['크기 는 어느 정도 되요 \\n']\n",
            "a:  ['\\t 이 거 는 약간 사이즈 가 커요 \\n']\n",
            "p:  ['이 거 는 15 예요 \\n']\n",
            "_\n",
            "q:  ['이 거 는 이에요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['가죽 아니고 쎄 무예 요 \\n']\n",
            "_\n",
            "q:  ['밍크 조끼 에 달려있는 거 보세요 \\n']\n",
            "a:  ['\\t 그건 집 에 있어요 \\n']\n",
            "p:  ['네 240 가 좀 나와요 \\n']\n",
            "_\n",
            "q:  ['조금 전 에 돼지 그거 얼마 예요 \\n']\n",
            "a:  ['\\t 5천 원요 \\n']\n",
            "p:  ['이 거 는 삼만 오천 원 이요 \\n']\n",
            "_\n",
            "q:  ['별로 예요 부엉이 이건 얼마 예요 \\n']\n",
            "a:  ['\\t 제품 밑 에 가격 이 있어요 \\n']\n",
            "p:  ['3만 원 정도 해 요 \\n']\n",
            "_\n",
            "q:  ['귀걸이 가 은 으로 된 건 없어요 \\n']\n",
            "a:  ['\\t 실버 적힌 거 는 다 은침 이에요 \\n']\n",
            "p:  ['정식 은 이 일 코너 가 있어요 \\n']\n",
            "_\n",
            "q:  ['골드 가 더 나아요 \\n']\n",
            "a:  ['\\t 골드 가 더 잘 거 같아요 \\n']\n",
            "p:  ['아뇨 \\n']\n",
            "_\n",
            "q:  ['에이 에스 비용 따로 돼요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['이 거 는 뒤 에 꽂는 게 은 으로 되어 있어요 \\n']\n",
            "a:  ['\\t 은침 이에요 \\n']\n",
            "p:  ['저쪽 있어요 \\n']\n",
            "_\n",
            "q:  ['이 거 는 큐빅 이에요 \\n']\n",
            "a:  ['\\t 네 그거 큐빅 이에요 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['이 핀 좀 해봐도 돼요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['이 거 는 지금 얼마 에요 \\n']\n",
            "a:  ['\\t 구천원 이 요 \\n']\n",
            "p:  ['오십 프로 세 일해 서 이십일만 구천 원 이에요 \\n']\n",
            "_\n",
            "q:  ['이 거 는 머리띠 예요 \\n']\n",
            "a:  ['\\t 그거 는 초 목 에 하는 거 예요 \\n']\n",
            "p:  ['그거 는 다 가죽 이에요 \\n']\n",
            "_\n",
            "q:  ['요 거 는 얼마 에요 \\n']\n",
            "a:  ['\\t 그거 구천원 이에요 \\n']\n",
            "p:  ['이 거 는 세 일 하 면 36만 원 이에요 \\n']\n",
            "_\n",
            "q:  ['큐빅 같은 거 하면 잘 빠지지 않아요 \\n']\n",
            "a:  ['\\t 이 거 는 큐빅 이 전체 라서 잘 빠지지는 않을 거 예요 \\n']\n",
            "p:  ['그냥 좀 좋아요 \\n']\n",
            "_\n",
            "q:  ['요즘 은 어떤 거 많이 해 요 \\n']\n",
            "a:  ['\\t 이런 것 도 잘 나가요 \\n']\n",
            "p:  ['출퇴근 용 으로는 이런 거 많이 해 요 \\n']\n",
            "_\n",
            "q:  ['이 거 는 약간 붉은 \\n']\n",
            "a:  ['\\t 이 거 는 컬러 예요 \\n']\n",
            "p:  ['그거 는 기비 제품 이 예요 \\n']\n",
            "_\n",
            "q:  ['구천원 에서 더 수 는 없어요 \\n']\n",
            "a:  ['\\t 싸게 파는 거 라서 디시 는 안 돼요 \\n']\n",
            "p:  ['30 하시면 3만 5천 원 까지 해드릴게요 \\n']\n",
            "_\n",
            "q:  ['혹시 세 일해 요 \\n']\n",
            "a:  ['\\t 아니요 \\n']\n",
            "p:  ['세 일 하 는 것 도 있어요 \\n']\n",
            "_\n",
            "q:  ['보 때 는 어떻게 해야 돼요 \\n']\n",
            "a:  ['\\t 별도 로 비닐 에 깔끔한 상태 로 보관 해주는 게 좋아요 \\n']\n",
            "p:  ['세탁소 에 고등학교 맡기는 거 예요 \\n']\n",
            "_\n",
            "q:  ['이 거 는 얼 만 데 요 \\n']\n",
            "a:  ['\\t 구천원 이 요 \\n']\n",
            "p:  ['이 거 처음 에 나왔을 때 십이만 원 받은 건데 팔만 원 이요 \\n']\n",
            "_\n",
            "q:  ['팔찌 종류 어디 있어요 \\n']\n",
            "a:  ['\\t 팔찌 는 이 쪽 에도 있고 여기 두 줄 다 팔찌 에요 \\n']\n",
            "p:  ['좌측 으로 가시 면 돼요 \\n']\n",
            "_\n",
            "q:  ['이 거 껴 봐도 돼요 \\n']\n",
            "a:  ['\\t 네 착용 가능하세요 \\n']\n",
            "p:  ['네 거울 은 이 쪽 에 있습니다 \\n']\n",
            "_\n",
            "q:  ['여기 요 \\n']\n",
            "a:  ['\\t 요 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['가격 은 얼마 에요 \\n']\n",
            "a:  ['\\t 이 거 는 삼만 팔천 원 이에요 \\n']\n",
            "p:  ['지금 할인 기간 이라 10만원 이세요 \\n']\n",
            "_\n",
            "q:  ['이 거 알 이 빠지면 에이 에스 는 가능해요 \\n']\n",
            "a:  ['\\t 저희 제품 은 에이 에스 는 따로 안되세요 \\n']\n",
            "p:  ['제 가 18 k 로 하고 있어요 \\n']\n",
            "_\n",
            "q:  ['세 일 하 는 거 볼게요 \\n']\n",
            "a:  ['\\t 요것 도 되게 괜찮고 이 것 도 다이아 가 들어가서 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['여기 가 여기 보다 좀 비싼 가요 \\n']\n",
            "a:  ['\\t 아니요 가격 대가 다 섞여 있어요 \\n']\n",
            "p:  ['네 충분합니다 \\n']\n",
            "_\n",
            "q:  ['리본 핀 이 요새 유행 인가 보네 \\n']\n",
            "a:  ['\\t 네 유행 안타 고 잘 나가는 게 리본 핀 이에요 \\n']\n",
            "p:  ['네 고객 님 이 거 는 신상 입니다 \\n']\n",
            "_\n",
            "q:  ['이 거 는 얼마 예요 \\n']\n",
            "a:  ['\\t 만 원 이고 이 거 는 만 원 이에요 \\n']\n",
            "p:  ['이 거 는 가죽 이라도 십구만 팔천 원 입니다 \\n']\n",
            "_\n",
            "q:  ['이 거 한 번 해볼 수 있어요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['네 맞아요 \\n']\n",
            "_\n",
            "q:  ['을 때 는 그냥 위로 되나요 \\n']\n",
            "a:  ['\\t 아니요 야 돼요 \\n']\n",
            "p:  ['기모 라 별로 되고 환불 과 하셔도 돼요 \\n']\n",
            "_\n",
            "q:  ['이 거 는 큐빅 인가요 \\n']\n",
            "a:  ['\\t 네 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['목걸이 도 다 천원 이에요 \\n']\n",
            "a:  ['\\t 네 천 원 이에요 \\n']\n",
            "p:  ['네 전부 다 통 이 좋아요 \\n']\n",
            "_\n",
            "q:  ['그것 도 끼워 봐도 돼요 \\n']\n",
            "a:  ['\\t 그거 는 돼요 착용 은 안 돼요 \\n']\n",
            "p:  ['네 \\n']\n",
            "_\n",
            "q:  ['왜 이렇게 싸요 이 거 는 재질 이 뭐 에요 \\n']\n",
            "a:  ['\\t 천 큐빅 을 이중 으로 붙여 나온 거 예요 \\n']\n",
            "p:  ['이건 옷 은 색깔 이라 거 는 안 돼요 \\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LMXJpkV_rG94"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "\bSeq2Seq와 Attention 메커니즘의 성능 비교.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e43db610dcf04a50826778d6def1bfae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_61761defb897442f8a7979daeef2172e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_48f3a6a5e4ab47a1958fba7cc7b6df41",
              "IPY_MODEL_041ece6dd758459089120d9d2c9ab8d4",
              "IPY_MODEL_880c5f81b73045c09dcf8755ed1d3b08"
            ]
          }
        },
        "61761defb897442f8a7979daeef2172e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48f3a6a5e4ab47a1958fba7cc7b6df41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_547181f53a154b6589bd7e3867082239",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3bb50ba2d0e743b2956aa8ea24a40b75"
          }
        },
        "041ece6dd758459089120d9d2c9ab8d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f8c7b9fb1ebb482184a14c62736acdbd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 200,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10628409eadc44c2b1fbde57c83d0b8b"
          }
        },
        "880c5f81b73045c09dcf8755ed1d3b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dc66ba98a2424e8aa5c1e4550e5b3060",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 200/200 [58:29&lt;00:00, 17.48s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c8242b5621db43818af3c893c57f4c06"
          }
        },
        "547181f53a154b6589bd7e3867082239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3bb50ba2d0e743b2956aa8ea24a40b75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8c7b9fb1ebb482184a14c62736acdbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10628409eadc44c2b1fbde57c83d0b8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc66ba98a2424e8aa5c1e4550e5b3060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c8242b5621db43818af3c893c57f4c06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}