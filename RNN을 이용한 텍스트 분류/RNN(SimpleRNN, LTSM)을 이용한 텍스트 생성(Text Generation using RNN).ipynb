{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN(SimpleRNN, LTSM)을 이용한 텍스트 생성(Text Generation using RNN).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYcwHMpOyrb2"
      },
      "source": [
        "참고 문헌: 딥 러닝을 이용한 자연어 처리 입문(https://wikidocs.net/45101)\n",
        "\n",
        "## RNN을 이용한 텍스트 생성(Text Generation using RNN)\n",
        "- 다 대 일(many-to-one) 구조의 RNN을 사용하여 문맥을 반영해서 텍스트를 생성하는 모델을 만들어보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7emW0KgI49HZ"
      },
      "source": [
        "예를 들어서 '경마장에 있는 말이 뛰고 있다'와 '그의 말이 법이다'와 '가는 말이 고와야 오는 말이 곱다'라는 세 가지 문장이 있다고 해보자.\n",
        "\n",
        "모델이 문맥을 학습할 수 있도록 전체 문장의 앞의 단어들을 전부 고려하여 학습하도록 데이터를 재구성한다면, \t\n",
        "\n",
        "### X(data) / y(label)\n",
        "\n",
        "1\t경마장에\t/있는  \n",
        "\n",
        "2\t경마장에 있는\t/말이\n",
        "\n",
        "3\t경마장에 있는 말이\t/뛰고\n",
        "\n",
        "4\t경마장에 있는 말이 뛰고\t/있다\n",
        "\n",
        "5\t그의\t/말이\n",
        "\n",
        "6\t그의 말이\t/법이다\n",
        "\n",
        "7\t가는\t/말이\n",
        "\n",
        "8\t가는 말이\t/고와야\n",
        "\n",
        "9\t가는 말이 고와야\t/오는\n",
        "\n",
        "10\t가는 말이 고와야 오는\t/말이\n",
        "\n",
        "11\t가는 말이 고와야 오는 말이\t/곱다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmAgtGbA5wK-"
      },
      "source": [
        "## 1. SimpleRNN 이용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTtOWdaTzCl9"
      },
      "source": [
        "### 1) 데이터에 대한 이해와 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyoIfK8qzBFV"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxeJlFYiynbJ"
      },
      "source": [
        "text=\"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
        "그의 말이 법이다\\n\n",
        "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrsWt5-xzPIf"
      },
      "source": [
        "단어 집합을 생성하고 크기를 확인해보자\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4-CW20pzOiX",
        "outputId": "676885cf-6489-4700-9ea4-565d24a16d76"
      },
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "'''\n",
        "케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,\n",
        "케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에\n",
        "배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야하므로 미리 +1 선언 \n",
        "'''\n",
        "\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F13GwYJ8zMOO",
        "outputId": "b3af93e3-e1fa-440f-d26c-1111bf017f8e"
      },
      "source": [
        "# 각 단어와 단어에 부여된 정수 인덱스 출력\n",
        "print(t.word_index)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTZH1MKvzjkE",
        "outputId": "24ea5a3e-d68a-4a90-8c1a-1be5592c18cc"
      },
      "source": [
        "# 훈련 데이터 생성\n",
        "sequences = list()\n",
        "for line in text.split('\\n'): # \\n을 기준으로 문장 토큰화\n",
        "    encoded = t.texts_to_sequences([line])[0] # 정수 인코딩\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습에 사용할 샘플의 개수: 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6XykA3Hz2Jd",
        "outputId": "9328afcb-017b-4fe5-8f94-e4a4bc1bba4e"
      },
      "source": [
        "encoded = t.texts_to_sequences([text.split('\\n')[0]])[0]\n",
        "print(encoded)\n",
        "print(encoded[:2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2, 3, 1, 4, 5]\n",
            "[2, 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KIYIKp8zzC_",
        "outputId": "6eebb2f0-0aa7-48b4-caaf-97d67d6e4d96"
      },
      "source": [
        "print(sequences)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4awSfuiw7fLR"
      },
      "source": [
        "왜 저렇게 문장을 나눌까? \n",
        "\n",
        "예를 들어 **'경마장에 있는 말이 뛰고 있다'** 라는 문장 하나가 있을 때, 하나의 단어를 예측하기 위해 이전에 등장한 단어들을 모두 참고하는 것입니다.\n",
        "\n",
        "\n",
        "X / y\t\n",
        "1\t경마장에\t/있는\n",
        "\n",
        "2\t경마장에 있는\t/말이\n",
        "\n",
        "3\t경마장에 있는 말이\t/뛰고\n",
        "\n",
        "4\t경마장에 있는 말이 뛰고 /있다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVhz82u50JIM"
      },
      "source": [
        "위의 데이터는 아직 레이블로 사용될 단어를 분리하지 않은 훈련 데이터입니다. [2, 3]은 [경마장에, 있는]에 해당되며 [2, 3, 1]은 [경마장에, 있는, 말이]에 해당됩니다. 전체 훈련 데이터에 대해서 맨 우측에 있는 단어에 대해서만 레이블로 분리해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLSRpbQRz0WG",
        "outputId": "20c02c35-e81c-4e65-953e-cb051a9b58c3"
      },
      "source": [
        "# 전체 샘플 길이 일치\n",
        "# 길이가 가장 긴 샘플은 [8, 1, 9, 10, 1, 11]이며 길이는 6\n",
        "\n",
        "max_len=max(len(l) for l in sequences) # 모든 샘플에서 길이가 가장 긴 샘플의 길이 출력\n",
        "print('샘플의 최대 길이 : {}'.format(max_len))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플의 최대 길이 : 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEXiRBkI0YVJ"
      },
      "source": [
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usmnly_N0aRR",
        "outputId": "e1f77c64-0b00-42a6-ee20-410375561a25"
      },
      "source": [
        "print(sequences)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  2  3]\n",
            " [ 0  0  0  2  3  1]\n",
            " [ 0  0  2  3  1  4]\n",
            " [ 0  2  3  1  4  5]\n",
            " [ 0  0  0  0  6  1]\n",
            " [ 0  0  0  6  1  7]\n",
            " [ 0  0  0  0  8  1]\n",
            " [ 0  0  0  8  1  9]\n",
            " [ 0  0  8  1  9 10]\n",
            " [ 0  8  1  9 10  1]\n",
            " [ 8  1  9 10  1 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxMBeTw_0cdD"
      },
      "source": [
        "# 마지막 단어를 레이블로 분리\n",
        "# Numpy를 이용\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]\n",
        "# 리스트의 마지막 값을 제외하고 저장한 것은 X\n",
        "# 리스트의 마지막 값만 저장한 것은 y. 이는 레이블에 해당됨."
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITxRCoE40fyg",
        "outputId": "1f619969-20e9-4353-a73d-0e2c8b7efa6d"
      },
      "source": [
        "print(X)\n",
        "print(y)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  2]\n",
            " [ 0  0  0  2  3]\n",
            " [ 0  0  2  3  1]\n",
            " [ 0  2  3  1  4]\n",
            " [ 0  0  0  0  6]\n",
            " [ 0  0  0  6  1]\n",
            " [ 0  0  0  0  8]\n",
            " [ 0  0  0  8  1]\n",
            " [ 0  0  8  1  9]\n",
            " [ 0  8  1  9 10]\n",
            " [ 8  1  9 10  1]]\n",
            "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prP6yFAQ0mO9",
        "outputId": "22633221-a04c-4c62-e866-4786927acffb"
      },
      "source": [
        "# RNN 모델에 훈련 데이터를 훈련 시키기 전에 레이블에 대해서 원-핫 인코딩을 수행 \n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "print(y)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMu0BpsN00JP"
      },
      "source": [
        "### 2) 모델 설계하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGoG14rS3LW2"
      },
      "source": [
        "각 단어의 임베딩 벡터는 10차원을 가지고, 32의 은닉 상태 크기를 가지는 바닐라 RNN을 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbour32i0sUT",
        "outputId": "b992f2fb-3cdc-460c-97f3-ad9b7ec1a73b"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "# Embedding(vocabulary 크기, 임베딩 후 벡터 크기, 입력 시퀀스 길이)\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_len -1 )) # 레이블을 분리하였으므로 이제 X의 길이는 5\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # 원핫인코딩 필요\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1/1 - 1s - loss: 2.4696 - accuracy: 0.0909\n",
            "Epoch 2/200\n",
            "1/1 - 0s - loss: 2.4566 - accuracy: 0.1818\n",
            "Epoch 3/200\n",
            "1/1 - 0s - loss: 2.4434 - accuracy: 0.3636\n",
            "Epoch 4/200\n",
            "1/1 - 0s - loss: 2.4300 - accuracy: 0.4545\n",
            "Epoch 5/200\n",
            "1/1 - 0s - loss: 2.4163 - accuracy: 0.4545\n",
            "Epoch 6/200\n",
            "1/1 - 0s - loss: 2.4023 - accuracy: 0.4545\n",
            "Epoch 7/200\n",
            "1/1 - 0s - loss: 2.3879 - accuracy: 0.4545\n",
            "Epoch 8/200\n",
            "1/1 - 0s - loss: 2.3730 - accuracy: 0.4545\n",
            "Epoch 9/200\n",
            "1/1 - 0s - loss: 2.3576 - accuracy: 0.4545\n",
            "Epoch 10/200\n",
            "1/1 - 0s - loss: 2.3417 - accuracy: 0.4545\n",
            "Epoch 11/200\n",
            "1/1 - 0s - loss: 2.3251 - accuracy: 0.4545\n",
            "Epoch 12/200\n",
            "1/1 - 0s - loss: 2.3079 - accuracy: 0.4545\n",
            "Epoch 13/200\n",
            "1/1 - 0s - loss: 2.2899 - accuracy: 0.4545\n",
            "Epoch 14/200\n",
            "1/1 - 0s - loss: 2.2711 - accuracy: 0.4545\n",
            "Epoch 15/200\n",
            "1/1 - 0s - loss: 2.2516 - accuracy: 0.4545\n",
            "Epoch 16/200\n",
            "1/1 - 0s - loss: 2.2313 - accuracy: 0.4545\n",
            "Epoch 17/200\n",
            "1/1 - 0s - loss: 2.2101 - accuracy: 0.4545\n",
            "Epoch 18/200\n",
            "1/1 - 0s - loss: 2.1881 - accuracy: 0.4545\n",
            "Epoch 19/200\n",
            "1/1 - 0s - loss: 2.1654 - accuracy: 0.4545\n",
            "Epoch 20/200\n",
            "1/1 - 0s - loss: 2.1420 - accuracy: 0.4545\n",
            "Epoch 21/200\n",
            "1/1 - 0s - loss: 2.1179 - accuracy: 0.4545\n",
            "Epoch 22/200\n",
            "1/1 - 0s - loss: 2.0934 - accuracy: 0.4545\n",
            "Epoch 23/200\n",
            "1/1 - 0s - loss: 2.0684 - accuracy: 0.4545\n",
            "Epoch 24/200\n",
            "1/1 - 0s - loss: 2.0433 - accuracy: 0.4545\n",
            "Epoch 25/200\n",
            "1/1 - 0s - loss: 2.0180 - accuracy: 0.4545\n",
            "Epoch 26/200\n",
            "1/1 - 0s - loss: 1.9929 - accuracy: 0.4545\n",
            "Epoch 27/200\n",
            "1/1 - 0s - loss: 1.9680 - accuracy: 0.4545\n",
            "Epoch 28/200\n",
            "1/1 - 0s - loss: 1.9436 - accuracy: 0.4545\n",
            "Epoch 29/200\n",
            "1/1 - 0s - loss: 1.9198 - accuracy: 0.4545\n",
            "Epoch 30/200\n",
            "1/1 - 0s - loss: 1.8966 - accuracy: 0.4545\n",
            "Epoch 31/200\n",
            "1/1 - 0s - loss: 1.8742 - accuracy: 0.4545\n",
            "Epoch 32/200\n",
            "1/1 - 0s - loss: 1.8525 - accuracy: 0.4545\n",
            "Epoch 33/200\n",
            "1/1 - 0s - loss: 1.8313 - accuracy: 0.4545\n",
            "Epoch 34/200\n",
            "1/1 - 0s - loss: 1.8107 - accuracy: 0.4545\n",
            "Epoch 35/200\n",
            "1/1 - 0s - loss: 1.7903 - accuracy: 0.4545\n",
            "Epoch 36/200\n",
            "1/1 - 0s - loss: 1.7702 - accuracy: 0.4545\n",
            "Epoch 37/200\n",
            "1/1 - 0s - loss: 1.7501 - accuracy: 0.4545\n",
            "Epoch 38/200\n",
            "1/1 - 0s - loss: 1.7300 - accuracy: 0.4545\n",
            "Epoch 39/200\n",
            "1/1 - 0s - loss: 1.7100 - accuracy: 0.4545\n",
            "Epoch 40/200\n",
            "1/1 - 0s - loss: 1.6900 - accuracy: 0.4545\n",
            "Epoch 41/200\n",
            "1/1 - 0s - loss: 1.6701 - accuracy: 0.4545\n",
            "Epoch 42/200\n",
            "1/1 - 0s - loss: 1.6505 - accuracy: 0.4545\n",
            "Epoch 43/200\n",
            "1/1 - 0s - loss: 1.6312 - accuracy: 0.4545\n",
            "Epoch 44/200\n",
            "1/1 - 0s - loss: 1.6121 - accuracy: 0.4545\n",
            "Epoch 45/200\n",
            "1/1 - 0s - loss: 1.5933 - accuracy: 0.4545\n",
            "Epoch 46/200\n",
            "1/1 - 0s - loss: 1.5747 - accuracy: 0.5455\n",
            "Epoch 47/200\n",
            "1/1 - 0s - loss: 1.5561 - accuracy: 0.5455\n",
            "Epoch 48/200\n",
            "1/1 - 0s - loss: 1.5374 - accuracy: 0.5455\n",
            "Epoch 49/200\n",
            "1/1 - 0s - loss: 1.5187 - accuracy: 0.5455\n",
            "Epoch 50/200\n",
            "1/1 - 0s - loss: 1.4998 - accuracy: 0.5455\n",
            "Epoch 51/200\n",
            "1/1 - 0s - loss: 1.4807 - accuracy: 0.5455\n",
            "Epoch 52/200\n",
            "1/1 - 0s - loss: 1.4617 - accuracy: 0.5455\n",
            "Epoch 53/200\n",
            "1/1 - 0s - loss: 1.4427 - accuracy: 0.5455\n",
            "Epoch 54/200\n",
            "1/1 - 0s - loss: 1.4238 - accuracy: 0.5455\n",
            "Epoch 55/200\n",
            "1/1 - 0s - loss: 1.4051 - accuracy: 0.5455\n",
            "Epoch 56/200\n",
            "1/1 - 0s - loss: 1.3865 - accuracy: 0.5455\n",
            "Epoch 57/200\n",
            "1/1 - 0s - loss: 1.3681 - accuracy: 0.5455\n",
            "Epoch 58/200\n",
            "1/1 - 0s - loss: 1.3498 - accuracy: 0.5455\n",
            "Epoch 59/200\n",
            "1/1 - 0s - loss: 1.3315 - accuracy: 0.5455\n",
            "Epoch 60/200\n",
            "1/1 - 0s - loss: 1.3132 - accuracy: 0.5455\n",
            "Epoch 61/200\n",
            "1/1 - 0s - loss: 1.2949 - accuracy: 0.6364\n",
            "Epoch 62/200\n",
            "1/1 - 0s - loss: 1.2766 - accuracy: 0.6364\n",
            "Epoch 63/200\n",
            "1/1 - 0s - loss: 1.2584 - accuracy: 0.7273\n",
            "Epoch 64/200\n",
            "1/1 - 0s - loss: 1.2403 - accuracy: 0.7273\n",
            "Epoch 65/200\n",
            "1/1 - 0s - loss: 1.2224 - accuracy: 0.7273\n",
            "Epoch 66/200\n",
            "1/1 - 0s - loss: 1.2048 - accuracy: 0.7273\n",
            "Epoch 67/200\n",
            "1/1 - 0s - loss: 1.1873 - accuracy: 0.7273\n",
            "Epoch 68/200\n",
            "1/1 - 0s - loss: 1.1700 - accuracy: 0.7273\n",
            "Epoch 69/200\n",
            "1/1 - 0s - loss: 1.1529 - accuracy: 0.7273\n",
            "Epoch 70/200\n",
            "1/1 - 0s - loss: 1.1359 - accuracy: 0.7273\n",
            "Epoch 71/200\n",
            "1/1 - 0s - loss: 1.1192 - accuracy: 0.7273\n",
            "Epoch 72/200\n",
            "1/1 - 0s - loss: 1.1026 - accuracy: 0.7273\n",
            "Epoch 73/200\n",
            "1/1 - 0s - loss: 1.0862 - accuracy: 0.7273\n",
            "Epoch 74/200\n",
            "1/1 - 0s - loss: 1.0701 - accuracy: 0.7273\n",
            "Epoch 75/200\n",
            "1/1 - 0s - loss: 1.0542 - accuracy: 0.7273\n",
            "Epoch 76/200\n",
            "1/1 - 0s - loss: 1.0385 - accuracy: 0.7273\n",
            "Epoch 77/200\n",
            "1/1 - 0s - loss: 1.0230 - accuracy: 0.7273\n",
            "Epoch 78/200\n",
            "1/1 - 0s - loss: 1.0077 - accuracy: 0.7273\n",
            "Epoch 79/200\n",
            "1/1 - 0s - loss: 0.9925 - accuracy: 0.7273\n",
            "Epoch 80/200\n",
            "1/1 - 0s - loss: 0.9776 - accuracy: 0.7273\n",
            "Epoch 81/200\n",
            "1/1 - 0s - loss: 0.9629 - accuracy: 0.7273\n",
            "Epoch 82/200\n",
            "1/1 - 0s - loss: 0.9484 - accuracy: 0.7273\n",
            "Epoch 83/200\n",
            "1/1 - 0s - loss: 0.9341 - accuracy: 0.7273\n",
            "Epoch 84/200\n",
            "1/1 - 0s - loss: 0.9200 - accuracy: 0.7273\n",
            "Epoch 85/200\n",
            "1/1 - 0s - loss: 0.9061 - accuracy: 0.7273\n",
            "Epoch 86/200\n",
            "1/1 - 0s - loss: 0.8924 - accuracy: 0.7273\n",
            "Epoch 87/200\n",
            "1/1 - 0s - loss: 0.8789 - accuracy: 0.7273\n",
            "Epoch 88/200\n",
            "1/1 - 0s - loss: 0.8656 - accuracy: 0.7273\n",
            "Epoch 89/200\n",
            "1/1 - 0s - loss: 0.8525 - accuracy: 0.7273\n",
            "Epoch 90/200\n",
            "1/1 - 0s - loss: 0.8396 - accuracy: 0.7273\n",
            "Epoch 91/200\n",
            "1/1 - 0s - loss: 0.8269 - accuracy: 0.7273\n",
            "Epoch 92/200\n",
            "1/1 - 0s - loss: 0.8145 - accuracy: 0.7273\n",
            "Epoch 93/200\n",
            "1/1 - 0s - loss: 0.8022 - accuracy: 0.7273\n",
            "Epoch 94/200\n",
            "1/1 - 0s - loss: 0.7901 - accuracy: 0.7273\n",
            "Epoch 95/200\n",
            "1/1 - 0s - loss: 0.7783 - accuracy: 0.7273\n",
            "Epoch 96/200\n",
            "1/1 - 0s - loss: 0.7666 - accuracy: 0.7273\n",
            "Epoch 97/200\n",
            "1/1 - 0s - loss: 0.7552 - accuracy: 0.7273\n",
            "Epoch 98/200\n",
            "1/1 - 0s - loss: 0.7439 - accuracy: 0.7273\n",
            "Epoch 99/200\n",
            "1/1 - 0s - loss: 0.7329 - accuracy: 0.7273\n",
            "Epoch 100/200\n",
            "1/1 - 0s - loss: 0.7221 - accuracy: 0.7273\n",
            "Epoch 101/200\n",
            "1/1 - 0s - loss: 0.7114 - accuracy: 0.8182\n",
            "Epoch 102/200\n",
            "1/1 - 0s - loss: 0.7010 - accuracy: 0.8182\n",
            "Epoch 103/200\n",
            "1/1 - 0s - loss: 0.6908 - accuracy: 0.8182\n",
            "Epoch 104/200\n",
            "1/1 - 0s - loss: 0.6808 - accuracy: 0.8182\n",
            "Epoch 105/200\n",
            "1/1 - 0s - loss: 0.6709 - accuracy: 0.8182\n",
            "Epoch 106/200\n",
            "1/1 - 0s - loss: 0.6613 - accuracy: 0.8182\n",
            "Epoch 107/200\n",
            "1/1 - 0s - loss: 0.6518 - accuracy: 0.8182\n",
            "Epoch 108/200\n",
            "1/1 - 0s - loss: 0.6426 - accuracy: 0.8182\n",
            "Epoch 109/200\n",
            "1/1 - 0s - loss: 0.6335 - accuracy: 0.8182\n",
            "Epoch 110/200\n",
            "1/1 - 0s - loss: 0.6246 - accuracy: 0.8182\n",
            "Epoch 111/200\n",
            "1/1 - 0s - loss: 0.6158 - accuracy: 0.9091\n",
            "Epoch 112/200\n",
            "1/1 - 0s - loss: 0.6073 - accuracy: 0.9091\n",
            "Epoch 113/200\n",
            "1/1 - 0s - loss: 0.5988 - accuracy: 0.9091\n",
            "Epoch 114/200\n",
            "1/1 - 0s - loss: 0.5906 - accuracy: 0.9091\n",
            "Epoch 115/200\n",
            "1/1 - 0s - loss: 0.5825 - accuracy: 0.9091\n",
            "Epoch 116/200\n",
            "1/1 - 0s - loss: 0.5745 - accuracy: 0.9091\n",
            "Epoch 117/200\n",
            "1/1 - 0s - loss: 0.5667 - accuracy: 0.9091\n",
            "Epoch 118/200\n",
            "1/1 - 0s - loss: 0.5591 - accuracy: 0.9091\n",
            "Epoch 119/200\n",
            "1/1 - 0s - loss: 0.5516 - accuracy: 0.9091\n",
            "Epoch 120/200\n",
            "1/1 - 0s - loss: 0.5442 - accuracy: 0.9091\n",
            "Epoch 121/200\n",
            "1/1 - 0s - loss: 0.5369 - accuracy: 0.9091\n",
            "Epoch 122/200\n",
            "1/1 - 0s - loss: 0.5298 - accuracy: 0.9091\n",
            "Epoch 123/200\n",
            "1/1 - 0s - loss: 0.5228 - accuracy: 0.9091\n",
            "Epoch 124/200\n",
            "1/1 - 0s - loss: 0.5160 - accuracy: 0.9091\n",
            "Epoch 125/200\n",
            "1/1 - 0s - loss: 0.5092 - accuracy: 0.9091\n",
            "Epoch 126/200\n",
            "1/1 - 0s - loss: 0.5026 - accuracy: 0.9091\n",
            "Epoch 127/200\n",
            "1/1 - 0s - loss: 0.4961 - accuracy: 0.9091\n",
            "Epoch 128/200\n",
            "1/1 - 0s - loss: 0.4896 - accuracy: 0.9091\n",
            "Epoch 129/200\n",
            "1/1 - 0s - loss: 0.4833 - accuracy: 0.9091\n",
            "Epoch 130/200\n",
            "1/1 - 0s - loss: 0.4772 - accuracy: 0.9091\n",
            "Epoch 131/200\n",
            "1/1 - 0s - loss: 0.4711 - accuracy: 0.9091\n",
            "Epoch 132/200\n",
            "1/1 - 0s - loss: 0.4651 - accuracy: 0.9091\n",
            "Epoch 133/200\n",
            "1/1 - 0s - loss: 0.4592 - accuracy: 0.9091\n",
            "Epoch 134/200\n",
            "1/1 - 0s - loss: 0.4534 - accuracy: 0.9091\n",
            "Epoch 135/200\n",
            "1/1 - 0s - loss: 0.4477 - accuracy: 0.9091\n",
            "Epoch 136/200\n",
            "1/1 - 0s - loss: 0.4422 - accuracy: 0.9091\n",
            "Epoch 137/200\n",
            "1/1 - 0s - loss: 0.4367 - accuracy: 0.9091\n",
            "Epoch 138/200\n",
            "1/1 - 0s - loss: 0.4313 - accuracy: 0.9091\n",
            "Epoch 139/200\n",
            "1/1 - 0s - loss: 0.4259 - accuracy: 0.9091\n",
            "Epoch 140/200\n",
            "1/1 - 0s - loss: 0.4207 - accuracy: 0.9091\n",
            "Epoch 141/200\n",
            "1/1 - 0s - loss: 0.4156 - accuracy: 0.9091\n",
            "Epoch 142/200\n",
            "1/1 - 0s - loss: 0.4105 - accuracy: 0.9091\n",
            "Epoch 143/200\n",
            "1/1 - 0s - loss: 0.4056 - accuracy: 0.9091\n",
            "Epoch 144/200\n",
            "1/1 - 0s - loss: 0.4007 - accuracy: 0.9091\n",
            "Epoch 145/200\n",
            "1/1 - 0s - loss: 0.3958 - accuracy: 0.9091\n",
            "Epoch 146/200\n",
            "1/1 - 0s - loss: 0.3911 - accuracy: 0.9091\n",
            "Epoch 147/200\n",
            "1/1 - 0s - loss: 0.3864 - accuracy: 0.9091\n",
            "Epoch 148/200\n",
            "1/1 - 0s - loss: 0.3818 - accuracy: 0.9091\n",
            "Epoch 149/200\n",
            "1/1 - 0s - loss: 0.3773 - accuracy: 0.9091\n",
            "Epoch 150/200\n",
            "1/1 - 0s - loss: 0.3729 - accuracy: 0.9091\n",
            "Epoch 151/200\n",
            "1/1 - 0s - loss: 0.3685 - accuracy: 0.9091\n",
            "Epoch 152/200\n",
            "1/1 - 0s - loss: 0.3642 - accuracy: 0.9091\n",
            "Epoch 153/200\n",
            "1/1 - 0s - loss: 0.3599 - accuracy: 0.9091\n",
            "Epoch 154/200\n",
            "1/1 - 0s - loss: 0.3557 - accuracy: 0.9091\n",
            "Epoch 155/200\n",
            "1/1 - 0s - loss: 0.3516 - accuracy: 0.9091\n",
            "Epoch 156/200\n",
            "1/1 - 0s - loss: 0.3475 - accuracy: 0.9091\n",
            "Epoch 157/200\n",
            "1/1 - 0s - loss: 0.3435 - accuracy: 0.9091\n",
            "Epoch 158/200\n",
            "1/1 - 0s - loss: 0.3395 - accuracy: 0.9091\n",
            "Epoch 159/200\n",
            "1/1 - 0s - loss: 0.3356 - accuracy: 0.9091\n",
            "Epoch 160/200\n",
            "1/1 - 0s - loss: 0.3317 - accuracy: 0.9091\n",
            "Epoch 161/200\n",
            "1/1 - 0s - loss: 0.3279 - accuracy: 0.9091\n",
            "Epoch 162/200\n",
            "1/1 - 0s - loss: 0.3241 - accuracy: 0.9091\n",
            "Epoch 163/200\n",
            "1/1 - 0s - loss: 0.3204 - accuracy: 0.9091\n",
            "Epoch 164/200\n",
            "1/1 - 0s - loss: 0.3167 - accuracy: 0.9091\n",
            "Epoch 165/200\n",
            "1/1 - 0s - loss: 0.3131 - accuracy: 0.9091\n",
            "Epoch 166/200\n",
            "1/1 - 0s - loss: 0.3095 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "1/1 - 0s - loss: 0.3059 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "1/1 - 0s - loss: 0.3024 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "1/1 - 0s - loss: 0.2989 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "1/1 - 0s - loss: 0.2954 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "1/1 - 0s - loss: 0.2920 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "1/1 - 0s - loss: 0.2886 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "1/1 - 0s - loss: 0.2852 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "1/1 - 0s - loss: 0.2819 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "1/1 - 0s - loss: 0.2786 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "1/1 - 0s - loss: 0.2753 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "1/1 - 0s - loss: 0.2720 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "1/1 - 0s - loss: 0.2688 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "1/1 - 0s - loss: 0.2656 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "1/1 - 0s - loss: 0.2624 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "1/1 - 0s - loss: 0.2592 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "1/1 - 0s - loss: 0.2560 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "1/1 - 0s - loss: 0.2529 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "1/1 - 0s - loss: 0.2498 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "1/1 - 0s - loss: 0.2467 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "1/1 - 0s - loss: 0.2436 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "1/1 - 0s - loss: 0.2405 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "1/1 - 0s - loss: 0.2374 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "1/1 - 0s - loss: 0.2344 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "1/1 - 0s - loss: 0.2313 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "1/1 - 0s - loss: 0.2283 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "1/1 - 0s - loss: 0.2253 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "1/1 - 0s - loss: 0.2223 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "1/1 - 0s - loss: 0.2193 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "1/1 - 0s - loss: 0.2163 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "1/1 - 0s - loss: 0.2133 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "1/1 - 0s - loss: 0.2103 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "1/1 - 0s - loss: 0.2073 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "1/1 - 0s - loss: 0.2044 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "1/1 - 0s - loss: 0.2014 - accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fde9db3fc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW_sRfUM4Mkv"
      },
      "source": [
        "모델이 정확하게 예측하고 있는지 문장을 생성하는 함수를 만들어서 출력해보자"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpsSswr83IgN"
      },
      "source": [
        "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
        "    sentence = ''\n",
        "    for _ in range(n): # n번 반복\n",
        "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
        "        encoded = pad_sequences([encoded], maxlen=5, padding='pre') # 데이터에 대한 패딩\n",
        "        result = np.argmax(model.predict(encoded), axis=-1)    # 입력한 X(현재 단어)에 대해서 Y를 예측하고 Y(예측한 단어)를 result에 저장.\n",
        "        for word, index in t.word_index.items(): \n",
        "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
        "                break # 해당 단어가 예측 단어이므로 break\n",
        "        current_word = current_word + ' ' + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
        "    # for문이므로 이 행동을 다시 반복\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GiW0V8g4pdT",
        "outputId": "4ad43142-b2d6-48c3-ff81-9c1935b69c6e"
      },
      "source": [
        "print(sentence_generation(model, t, '경마장에', 4))\n",
        "# '경마장에' 라는 단어 뒤에는 총 4개의 단어가 있으므로 4번 예측"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "경마장에 있는 말이 뛰고 있다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8p9GxxC4qUh",
        "outputId": "6b3a7a2d-5a64-4cf0-ca0f-bc61e4b5b480"
      },
      "source": [
        "print(sentence_generation(model, t, '그의', 2)) # 2번 예측"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "그의 말이 법이다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe_KIjXX41rj",
        "outputId": "b3a32bae-9aec-4e61-c295-eb4a6fa28bfd"
      },
      "source": [
        "print(sentence_generation(model, t, '가는', 5)) # 5번 예측"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "가는 말이 고와야 오는 말이 곱다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oIU_QQG5YTU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHbpiJIk51D4"
      },
      "source": [
        "## 2. LSTM 이용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urE1B8_r55SJ"
      },
      "source": [
        "### 1) 데이터에 대한 이해와 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0drlvqWn6VqY"
      },
      "source": [
        "사용할 데이터는 뉴욕 타임즈 기사의 제목 ArticlesApril2018.csv \n",
        "파일 다운로드 링크 : https://www.kaggle.com/aashita/nyt-comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtg0p7KQ52Hz"
      },
      "source": [
        "import os\n",
        "\n",
        "# os.environ을 이용하여 Kaggle API Username, Key 세팅하기\n",
        "\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UGGlMEX6bCK",
        "outputId": "5d21175c-d3e1-4b05-e93f-7502290f0738"
      },
      "source": [
        "!kaggle datasets download -d aashita/nyt-comments\n",
        "!unzip '*.zip'"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nyt-comments.zip to /content\n",
            " 99% 473M/480M [00:09<00:00, 60.4MB/s]\n",
            "100% 480M/480M [00:09<00:00, 53.2MB/s]\n",
            "Archive:  nyt-comments.zip\n",
            "  inflating: ArticlesApril2017.csv   \n",
            "  inflating: ArticlesApril2018.csv   \n",
            "  inflating: ArticlesFeb2017.csv     \n",
            "  inflating: ArticlesFeb2018.csv     \n",
            "  inflating: ArticlesJan2017.csv     \n",
            "  inflating: ArticlesJan2018.csv     \n",
            "  inflating: ArticlesMarch2017.csv   \n",
            "  inflating: ArticlesMarch2018.csv   \n",
            "  inflating: ArticlesMay2017.csv     \n",
            "  inflating: CommentsApril2017.csv   \n",
            "  inflating: CommentsApril2018.csv   \n",
            "  inflating: CommentsFeb2017.csv     \n",
            "  inflating: CommentsFeb2018.csv     \n",
            "  inflating: CommentsJan2017.csv     \n",
            "  inflating: CommentsJan2018.csv     \n",
            "  inflating: CommentsMarch2017.csv   \n",
            "  inflating: CommentsMarch2018.csv   \n",
            "  inflating: CommentsMay2017.csv     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "164isA4q6iRk",
        "outputId": "459dca6e-45b1-4476-d554-a028eacd87dd"
      },
      "source": [
        "import pandas as pd\n",
        "from string import punctuation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "df = pd.read_csv('ArticlesApril2018.csv')\n",
        "df.head()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>articleID</th>\n",
              "      <th>articleWordCount</th>\n",
              "      <th>byline</th>\n",
              "      <th>documentType</th>\n",
              "      <th>headline</th>\n",
              "      <th>keywords</th>\n",
              "      <th>multimedia</th>\n",
              "      <th>newDesk</th>\n",
              "      <th>printPage</th>\n",
              "      <th>pubDate</th>\n",
              "      <th>sectionName</th>\n",
              "      <th>snippet</th>\n",
              "      <th>source</th>\n",
              "      <th>typeOfMaterial</th>\n",
              "      <th>webURL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5adf6684068401528a2aa69b</td>\n",
              "      <td>781</td>\n",
              "      <td>By JOHN BRANCH</td>\n",
              "      <td>article</td>\n",
              "      <td>Former N.F.L. Cheerleaders’ Settlement Offer: ...</td>\n",
              "      <td>['Workplace Hazards and Violations', 'Football...</td>\n",
              "      <td>68</td>\n",
              "      <td>Sports</td>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-24 17:16:49</td>\n",
              "      <td>Pro Football</td>\n",
              "      <td>“I understand that they could meet with us, pa...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2018/04/24/sports/foot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5adf653f068401528a2aa697</td>\n",
              "      <td>656</td>\n",
              "      <td>By LISA FRIEDMAN</td>\n",
              "      <td>article</td>\n",
              "      <td>E.P.A. to Unveil a New Rule. Its Effect: Less ...</td>\n",
              "      <td>['Environmental Protection Agency', 'Pruitt, S...</td>\n",
              "      <td>68</td>\n",
              "      <td>Climate</td>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-24 17:11:21</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>The agency plans to publish a new regulation T...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2018/04/24/climate/epa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5adf4626068401528a2aa628</td>\n",
              "      <td>2427</td>\n",
              "      <td>By PETE WELLS</td>\n",
              "      <td>article</td>\n",
              "      <td>The New Noma, Explained</td>\n",
              "      <td>['Restaurants', 'Noma (Copenhagen, Restaurant)...</td>\n",
              "      <td>66</td>\n",
              "      <td>Dining</td>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-24 14:58:44</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>What’s it like to eat at the second incarnatio...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2018/04/24/dining/noma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5adf40d2068401528a2aa619</td>\n",
              "      <td>626</td>\n",
              "      <td>By JULIE HIRSCHFELD DAVIS and PETER BAKER</td>\n",
              "      <td>article</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>['Macron, Emmanuel (1977- )', 'Trump, Donald J...</td>\n",
              "      <td>68</td>\n",
              "      <td>Washington</td>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-24 14:35:57</td>\n",
              "      <td>Europe</td>\n",
              "      <td>President Trump welcomed President Emmanuel Ma...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2018/04/24/world/europ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5adf3d64068401528a2aa60f</td>\n",
              "      <td>815</td>\n",
              "      <td>By IAN AUSTEN and DAN BILEFSKY</td>\n",
              "      <td>article</td>\n",
              "      <td>Unknown</td>\n",
              "      <td>['Toronto, Ontario, Attack (April, 2018)', 'Mu...</td>\n",
              "      <td>68</td>\n",
              "      <td>Foreign</td>\n",
              "      <td>0</td>\n",
              "      <td>2018-04-24 14:21:21</td>\n",
              "      <td>Canada</td>\n",
              "      <td>Alek Minassian, 25, a resident of Toronto’s Ri...</td>\n",
              "      <td>The New York Times</td>\n",
              "      <td>News</td>\n",
              "      <td>https://www.nytimes.com/2018/04/24/world/canad...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  articleID  ...                                             webURL\n",
              "0  5adf6684068401528a2aa69b  ...  https://www.nytimes.com/2018/04/24/sports/foot...\n",
              "1  5adf653f068401528a2aa697  ...  https://www.nytimes.com/2018/04/24/climate/epa...\n",
              "2  5adf4626068401528a2aa628  ...  https://www.nytimes.com/2018/04/24/dining/noma...\n",
              "3  5adf40d2068401528a2aa619  ...  https://www.nytimes.com/2018/04/24/world/europ...\n",
              "4  5adf3d64068401528a2aa60f  ...  https://www.nytimes.com/2018/04/24/world/canad...\n",
              "\n",
              "[5 rows x 15 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHcEgWu66qow",
        "outputId": "09e4f59e-ed81-48b9-de30-4214adf9ec5a"
      },
      "source": [
        "print('열의 개수: ',len(df.columns))\n",
        "print(df.columns)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "열의 개수:  15\n",
            "Index(['articleID', 'articleWordCount', 'byline', 'documentType', 'headline',\n",
            "       'keywords', 'multimedia', 'newDesk', 'printPage', 'pubDate',\n",
            "       'sectionName', 'snippet', 'source', 'typeOfMaterial', 'webURL'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U31dc4006weW",
        "outputId": "e7b4c260-42da-4faa-dde8-d85d0f468aac"
      },
      "source": [
        "df['headline'].isnull().values.any()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKInSWQb6xYd",
        "outputId": "45b976e9-ee66-412b-c63b-125dd48dced9"
      },
      "source": [
        "# headline 열에서 모든 신문 기사의 제목을 뽑아서 하나의 리스트로 저장\n",
        "\n",
        "headline = [] # 리스트 선언\n",
        "headline.extend(list(df.headline.values)) # 헤드라인의 값들을 리스트로 저장\n",
        "headline[:5] # 상위 5개만 출력"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
              " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
              " 'The New Noma, Explained',\n",
              " 'Unknown',\n",
              " 'Unknown']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYi5Zq4a66TZ",
        "outputId": "1a31e10a-8494-49c5-817f-f4c56a718a1c"
      },
      "source": [
        "# Unknown 값 -> 노이즈 데이터이므로 제거해줄 필요있음\n",
        "\n",
        "print('총 샘플의 개수 : {}'.format(len(headline))) # 현재 샘플의 개수\n",
        "headline = [n for n in headline if n != \"Unknown\"] # Unknown 값을 가진 샘플 제거\n",
        "print('노이즈값 제거 후 샘플의 개수 : {}'.format(len(headline))) # 제거 후 샘플의 개수"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "총 샘플의 개수 : 1324\n",
            "노이즈값 제거 후 샘플의 개수 : 1214\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eFw1CPp7DpZ",
        "outputId": "397181d3-11eb-47c0-f11a-208cc96f8970"
      },
      "source": [
        "headline[:5]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Former N.F.L. Cheerleaders’ Settlement Offer: $1 and a Meeting With Goodell',\n",
              " 'E.P.A. to Unveil a New Rule. Its Effect: Less Science in Policymaking.',\n",
              " 'The New Noma, Explained',\n",
              " 'How a Bag of Texas Dirt  Became a Times Tradition',\n",
              " 'Is School a Place for Self-Expression?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKNTO-Nw61dd",
        "outputId": "2b4c0197-8a5e-467a-b03a-eb86e3455218"
      },
      "source": [
        " # 전처리 수행하기\n",
        " # 구두점 제거, 단어의 소문자화\n",
        " \n",
        " def repreprocessing(s):\n",
        "    s=s.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return ''.join(c for c in s if c not in punctuation).lower() # 구두점 제거와 동시에 소문자화\n",
        "\n",
        "text = [repreprocessing(x) for x in headline]\n",
        "text[:5]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['former nfl cheerleaders settlement offer 1 and a meeting with goodell',\n",
              " 'epa to unveil a new rule its effect less science in policymaking',\n",
              " 'the new noma explained',\n",
              " 'how a bag of texas dirt  became a times tradition',\n",
              " 'is school a place for selfexpression']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJIjMKYf7L1Q",
        "outputId": "10382670-8cb4-41c1-fb69-48be302cea38"
      },
      "source": [
        "# 단어 집합 만들기\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(text)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 3494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVk3I37-7Ojn",
        "outputId": "92bbf6b0-b22d-4a87-cc63-304121823f18"
      },
      "source": [
        "# 훈련 데이터 구성\n",
        "\n",
        "sequences = list()\n",
        "\n",
        "for line in text: # 1,214 개의 샘플에 대해서 샘플을 1개씩 가져온다.\n",
        "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "sequences[:11] # 11개의 샘플 출력"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[99, 269],\n",
              " [99, 269, 371],\n",
              " [99, 269, 371, 1115],\n",
              " [99, 269, 371, 1115, 582],\n",
              " [99, 269, 371, 1115, 582, 52],\n",
              " [99, 269, 371, 1115, 582, 52, 7],\n",
              " [99, 269, 371, 1115, 582, 52, 7, 2],\n",
              " [99, 269, 371, 1115, 582, 52, 7, 2, 372],\n",
              " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10],\n",
              " [99, 269, 371, 1115, 582, 52, 7, 2, 372, 10, 1116],\n",
              " [100, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLp08H-K7jZ-",
        "outputId": "5ea41194-a27a-4ff1-946f-2e27b09ca948"
      },
      "source": [
        "# 어떤 정수가 어떤 단어를 의미하는지 알아보기 위해 인덱스로부터 단어를 찾는 index_to_word 만들기\n",
        "\n",
        "index_to_word={}\n",
        "for key, value in t.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
        "    index_to_word[value] = key\n",
        "\n",
        "print('빈도수 상위 582번 단어 : {}'.format(index_to_word[582]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "빈도수 상위 582번 단어 : offer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXwn_oK_7Ys3",
        "outputId": "74f0a06e-6cf2-441f-c053-4e0ef141aad8"
      },
      "source": [
        "# 패딩 작업\n",
        "max_len=max(len(l) for l in sequences)\n",
        "print('샘플의 최대 길이 : {}'.format(max_len))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "샘플의 최대 길이 : 24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkhXv5Zd8Gus",
        "outputId": "3a25f44a-eb3c-44c8-c4ff-a4fd6518ea78"
      },
      "source": [
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "print(sequences[:3])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0   99  269]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0   99  269  371]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0   99  269  371 1115]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wjGKfAb8IB4",
        "outputId": "ac6e0d40-118c-4fc0-e8b1-fb4ff2ed8615"
      },
      "source": [
        "# 레이블 분리\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]\n",
        "print(X[:3])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0  99]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0  99 269]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0  99 269 371]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFtPGF9l8KLQ",
        "outputId": "85c8b820-5097-4195-bdb5-843c39575f8d"
      },
      "source": [
        "print(y[:3]) # 레이블"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 269  371 1115]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1k5yLK58Qt6"
      },
      "source": [
        "# 원핫 인코딩\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XqZBO6l8Tap"
      },
      "source": [
        "### 2) 모델 설계하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6lcCIJ-8Se4",
        "outputId": "6a2b98d9-6df7-4265-bef1-6d129d9ef2e3"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "\n",
        "# 각 단어의 임베딩 벡터는 10차원을 가지고, 128의 은닉 상태 크기를 가지는 LSTM\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_len-1)) # y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=2)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "244/244 - 9s - loss: 7.6352 - accuracy: 0.0311\n",
            "Epoch 2/200\n",
            "244/244 - 7s - loss: 7.1152 - accuracy: 0.0292\n",
            "Epoch 3/200\n",
            "244/244 - 7s - loss: 6.9808 - accuracy: 0.0332\n",
            "Epoch 4/200\n",
            "244/244 - 6s - loss: 6.8552 - accuracy: 0.0417\n",
            "Epoch 5/200\n",
            "244/244 - 7s - loss: 6.7103 - accuracy: 0.0429\n",
            "Epoch 6/200\n",
            "244/244 - 7s - loss: 6.5464 - accuracy: 0.0452\n",
            "Epoch 7/200\n",
            "244/244 - 7s - loss: 6.3578 - accuracy: 0.0497\n",
            "Epoch 8/200\n",
            "244/244 - 7s - loss: 6.1593 - accuracy: 0.0551\n",
            "Epoch 9/200\n",
            "244/244 - 7s - loss: 5.9636 - accuracy: 0.0633\n",
            "Epoch 10/200\n",
            "244/244 - 7s - loss: 5.7772 - accuracy: 0.0641\n",
            "Epoch 11/200\n",
            "244/244 - 6s - loss: 5.5989 - accuracy: 0.0697\n",
            "Epoch 12/200\n",
            "244/244 - 6s - loss: 5.4348 - accuracy: 0.0728\n",
            "Epoch 13/200\n",
            "244/244 - 6s - loss: 5.2750 - accuracy: 0.0787\n",
            "Epoch 14/200\n",
            "244/244 - 6s - loss: 5.1258 - accuracy: 0.0852\n",
            "Epoch 15/200\n",
            "244/244 - 7s - loss: 4.9816 - accuracy: 0.0933\n",
            "Epoch 16/200\n",
            "244/244 - 7s - loss: 4.8398 - accuracy: 0.1015\n",
            "Epoch 17/200\n",
            "244/244 - 6s - loss: 4.7081 - accuracy: 0.1165\n",
            "Epoch 18/200\n",
            "244/244 - 7s - loss: 4.5794 - accuracy: 0.1278\n",
            "Epoch 19/200\n",
            "244/244 - 7s - loss: 4.4556 - accuracy: 0.1402\n",
            "Epoch 20/200\n",
            "244/244 - 7s - loss: 4.3352 - accuracy: 0.1584\n",
            "Epoch 21/200\n",
            "244/244 - 7s - loss: 4.2175 - accuracy: 0.1731\n",
            "Epoch 22/200\n",
            "244/244 - 7s - loss: 4.1062 - accuracy: 0.1898\n",
            "Epoch 23/200\n",
            "244/244 - 7s - loss: 3.9962 - accuracy: 0.2048\n",
            "Epoch 24/200\n",
            "244/244 - 7s - loss: 3.8878 - accuracy: 0.2271\n",
            "Epoch 25/200\n",
            "244/244 - 7s - loss: 3.7858 - accuracy: 0.2452\n",
            "Epoch 26/200\n",
            "244/244 - 7s - loss: 3.6815 - accuracy: 0.2623\n",
            "Epoch 27/200\n",
            "244/244 - 7s - loss: 3.5858 - accuracy: 0.2805\n",
            "Epoch 28/200\n",
            "244/244 - 7s - loss: 3.4910 - accuracy: 0.3012\n",
            "Epoch 29/200\n",
            "244/244 - 6s - loss: 3.3965 - accuracy: 0.3165\n",
            "Epoch 30/200\n",
            "244/244 - 6s - loss: 3.3053 - accuracy: 0.3386\n",
            "Epoch 31/200\n",
            "244/244 - 6s - loss: 3.2155 - accuracy: 0.3519\n",
            "Epoch 32/200\n",
            "244/244 - 6s - loss: 3.1302 - accuracy: 0.3699\n",
            "Epoch 33/200\n",
            "244/244 - 6s - loss: 3.0477 - accuracy: 0.3819\n",
            "Epoch 34/200\n",
            "244/244 - 6s - loss: 2.9653 - accuracy: 0.3972\n",
            "Epoch 35/200\n",
            "244/244 - 7s - loss: 2.8867 - accuracy: 0.4133\n",
            "Epoch 36/200\n",
            "244/244 - 7s - loss: 2.8098 - accuracy: 0.4295\n",
            "Epoch 37/200\n",
            "244/244 - 7s - loss: 2.7376 - accuracy: 0.4468\n",
            "Epoch 38/200\n",
            "244/244 - 7s - loss: 2.6646 - accuracy: 0.4580\n",
            "Epoch 39/200\n",
            "244/244 - 6s - loss: 2.5960 - accuracy: 0.4725\n",
            "Epoch 40/200\n",
            "244/244 - 7s - loss: 2.5282 - accuracy: 0.4856\n",
            "Epoch 41/200\n",
            "244/244 - 7s - loss: 2.4626 - accuracy: 0.4985\n",
            "Epoch 42/200\n",
            "244/244 - 7s - loss: 2.3999 - accuracy: 0.5119\n",
            "Epoch 43/200\n",
            "244/244 - 7s - loss: 2.3368 - accuracy: 0.5234\n",
            "Epoch 44/200\n",
            "244/244 - 7s - loss: 2.2780 - accuracy: 0.5351\n",
            "Epoch 45/200\n",
            "244/244 - 7s - loss: 2.2186 - accuracy: 0.5495\n",
            "Epoch 46/200\n",
            "244/244 - 7s - loss: 2.1635 - accuracy: 0.5586\n",
            "Epoch 47/200\n",
            "244/244 - 7s - loss: 2.1092 - accuracy: 0.5731\n",
            "Epoch 48/200\n",
            "244/244 - 7s - loss: 2.0542 - accuracy: 0.5905\n",
            "Epoch 49/200\n",
            "244/244 - 6s - loss: 1.9998 - accuracy: 0.5926\n",
            "Epoch 50/200\n",
            "244/244 - 7s - loss: 1.9608 - accuracy: 0.6009\n",
            "Epoch 51/200\n",
            "244/244 - 7s - loss: 1.9190 - accuracy: 0.6145\n",
            "Epoch 52/200\n",
            "244/244 - 7s - loss: 1.8584 - accuracy: 0.6242\n",
            "Epoch 53/200\n",
            "244/244 - 7s - loss: 1.8113 - accuracy: 0.6364\n",
            "Epoch 54/200\n",
            "244/244 - 7s - loss: 1.7676 - accuracy: 0.6439\n",
            "Epoch 55/200\n",
            "244/244 - 7s - loss: 1.7223 - accuracy: 0.6509\n",
            "Epoch 56/200\n",
            "244/244 - 7s - loss: 1.6810 - accuracy: 0.6615\n",
            "Epoch 57/200\n",
            "244/244 - 7s - loss: 1.6389 - accuracy: 0.6737\n",
            "Epoch 58/200\n",
            "244/244 - 7s - loss: 1.6049 - accuracy: 0.6719\n",
            "Epoch 59/200\n",
            "244/244 - 7s - loss: 1.5657 - accuracy: 0.6865\n",
            "Epoch 60/200\n",
            "244/244 - 7s - loss: 1.5261 - accuracy: 0.6972\n",
            "Epoch 61/200\n",
            "244/244 - 7s - loss: 1.4864 - accuracy: 0.7009\n",
            "Epoch 62/200\n",
            "244/244 - 7s - loss: 1.4529 - accuracy: 0.7091\n",
            "Epoch 63/200\n",
            "244/244 - 7s - loss: 1.4133 - accuracy: 0.7178\n",
            "Epoch 64/200\n",
            "244/244 - 7s - loss: 1.3842 - accuracy: 0.7236\n",
            "Epoch 65/200\n",
            "244/244 - 7s - loss: 1.3468 - accuracy: 0.7311\n",
            "Epoch 66/200\n",
            "244/244 - 7s - loss: 1.3197 - accuracy: 0.7343\n",
            "Epoch 67/200\n",
            "244/244 - 7s - loss: 1.2852 - accuracy: 0.7471\n",
            "Epoch 68/200\n",
            "244/244 - 7s - loss: 1.2530 - accuracy: 0.7509\n",
            "Epoch 69/200\n",
            "244/244 - 7s - loss: 1.2216 - accuracy: 0.7561\n",
            "Epoch 70/200\n",
            "244/244 - 7s - loss: 1.1948 - accuracy: 0.7628\n",
            "Epoch 71/200\n",
            "244/244 - 7s - loss: 1.1648 - accuracy: 0.7702\n",
            "Epoch 72/200\n",
            "244/244 - 7s - loss: 1.1379 - accuracy: 0.7735\n",
            "Epoch 73/200\n",
            "244/244 - 7s - loss: 1.1110 - accuracy: 0.7793\n",
            "Epoch 74/200\n",
            "244/244 - 7s - loss: 1.0847 - accuracy: 0.7844\n",
            "Epoch 75/200\n",
            "244/244 - 7s - loss: 1.0569 - accuracy: 0.7939\n",
            "Epoch 76/200\n",
            "244/244 - 7s - loss: 1.0374 - accuracy: 0.7951\n",
            "Epoch 77/200\n",
            "244/244 - 7s - loss: 1.0103 - accuracy: 0.8010\n",
            "Epoch 78/200\n",
            "244/244 - 7s - loss: 0.9870 - accuracy: 0.8016\n",
            "Epoch 79/200\n",
            "244/244 - 7s - loss: 0.9640 - accuracy: 0.8128\n",
            "Epoch 80/200\n",
            "244/244 - 7s - loss: 0.9402 - accuracy: 0.8129\n",
            "Epoch 81/200\n",
            "244/244 - 7s - loss: 0.9224 - accuracy: 0.8196\n",
            "Epoch 82/200\n",
            "244/244 - 7s - loss: 0.9041 - accuracy: 0.8255\n",
            "Epoch 83/200\n",
            "244/244 - 7s - loss: 0.8771 - accuracy: 0.8276\n",
            "Epoch 84/200\n",
            "244/244 - 7s - loss: 0.8566 - accuracy: 0.8319\n",
            "Epoch 85/200\n",
            "244/244 - 7s - loss: 0.8361 - accuracy: 0.8374\n",
            "Epoch 86/200\n",
            "244/244 - 7s - loss: 0.8173 - accuracy: 0.8390\n",
            "Epoch 87/200\n",
            "244/244 - 7s - loss: 0.8029 - accuracy: 0.8453\n",
            "Epoch 88/200\n",
            "244/244 - 7s - loss: 0.7813 - accuracy: 0.8492\n",
            "Epoch 89/200\n",
            "244/244 - 7s - loss: 0.7636 - accuracy: 0.8540\n",
            "Epoch 90/200\n",
            "244/244 - 7s - loss: 0.7463 - accuracy: 0.8565\n",
            "Epoch 91/200\n",
            "244/244 - 7s - loss: 0.7321 - accuracy: 0.8575\n",
            "Epoch 92/200\n",
            "244/244 - 7s - loss: 0.7165 - accuracy: 0.8616\n",
            "Epoch 93/200\n",
            "244/244 - 7s - loss: 0.6994 - accuracy: 0.8654\n",
            "Epoch 94/200\n",
            "244/244 - 7s - loss: 0.6842 - accuracy: 0.8672\n",
            "Epoch 95/200\n",
            "244/244 - 7s - loss: 0.6695 - accuracy: 0.8694\n",
            "Epoch 96/200\n",
            "244/244 - 7s - loss: 0.6534 - accuracy: 0.8715\n",
            "Epoch 97/200\n",
            "244/244 - 7s - loss: 0.6417 - accuracy: 0.8763\n",
            "Epoch 98/200\n",
            "244/244 - 7s - loss: 0.6261 - accuracy: 0.8785\n",
            "Epoch 99/200\n",
            "244/244 - 7s - loss: 0.6127 - accuracy: 0.8825\n",
            "Epoch 100/200\n",
            "244/244 - 7s - loss: 0.6026 - accuracy: 0.8839\n",
            "Epoch 101/200\n",
            "244/244 - 7s - loss: 0.5914 - accuracy: 0.8856\n",
            "Epoch 102/200\n",
            "244/244 - 7s - loss: 0.5803 - accuracy: 0.8872\n",
            "Epoch 103/200\n",
            "244/244 - 7s - loss: 0.5626 - accuracy: 0.8881\n",
            "Epoch 104/200\n",
            "244/244 - 7s - loss: 0.5511 - accuracy: 0.8925\n",
            "Epoch 105/200\n",
            "244/244 - 7s - loss: 0.5391 - accuracy: 0.8930\n",
            "Epoch 106/200\n",
            "244/244 - 7s - loss: 0.5290 - accuracy: 0.8947\n",
            "Epoch 107/200\n",
            "244/244 - 7s - loss: 0.5193 - accuracy: 0.8954\n",
            "Epoch 108/200\n",
            "244/244 - 7s - loss: 0.5165 - accuracy: 0.8976\n",
            "Epoch 109/200\n",
            "244/244 - 7s - loss: 0.5003 - accuracy: 0.8986\n",
            "Epoch 110/200\n",
            "244/244 - 7s - loss: 0.4890 - accuracy: 0.9007\n",
            "Epoch 111/200\n",
            "244/244 - 7s - loss: 0.4783 - accuracy: 0.9030\n",
            "Epoch 112/200\n",
            "244/244 - 7s - loss: 0.4764 - accuracy: 0.9025\n",
            "Epoch 113/200\n",
            "244/244 - 7s - loss: 0.4629 - accuracy: 0.9058\n",
            "Epoch 114/200\n",
            "244/244 - 7s - loss: 0.4535 - accuracy: 0.9075\n",
            "Epoch 115/200\n",
            "244/244 - 7s - loss: 0.4460 - accuracy: 0.9066\n",
            "Epoch 116/200\n",
            "244/244 - 7s - loss: 0.4368 - accuracy: 0.9085\n",
            "Epoch 117/200\n",
            "244/244 - 7s - loss: 0.4313 - accuracy: 0.9095\n",
            "Epoch 118/200\n",
            "244/244 - 7s - loss: 0.4281 - accuracy: 0.9088\n",
            "Epoch 119/200\n",
            "244/244 - 7s - loss: 0.4270 - accuracy: 0.9104\n",
            "Epoch 120/200\n",
            "244/244 - 7s - loss: 0.4103 - accuracy: 0.9136\n",
            "Epoch 121/200\n",
            "244/244 - 7s - loss: 0.4050 - accuracy: 0.9117\n",
            "Epoch 122/200\n",
            "244/244 - 7s - loss: 0.4132 - accuracy: 0.9113\n",
            "Epoch 123/200\n",
            "244/244 - 7s - loss: 0.3967 - accuracy: 0.9122\n",
            "Epoch 124/200\n",
            "244/244 - 7s - loss: 0.3852 - accuracy: 0.9127\n",
            "Epoch 125/200\n",
            "244/244 - 7s - loss: 0.3790 - accuracy: 0.9141\n",
            "Epoch 126/200\n",
            "244/244 - 7s - loss: 0.3737 - accuracy: 0.9140\n",
            "Epoch 127/200\n",
            "244/244 - 7s - loss: 0.3689 - accuracy: 0.9159\n",
            "Epoch 128/200\n",
            "244/244 - 7s - loss: 0.3653 - accuracy: 0.9155\n",
            "Epoch 129/200\n",
            "244/244 - 7s - loss: 0.3607 - accuracy: 0.9146\n",
            "Epoch 130/200\n",
            "244/244 - 7s - loss: 0.3554 - accuracy: 0.9173\n",
            "Epoch 131/200\n",
            "244/244 - 7s - loss: 0.3523 - accuracy: 0.9148\n",
            "Epoch 132/200\n",
            "244/244 - 7s - loss: 0.3487 - accuracy: 0.9154\n",
            "Epoch 133/200\n",
            "244/244 - 7s - loss: 0.3441 - accuracy: 0.9166\n",
            "Epoch 134/200\n",
            "244/244 - 7s - loss: 0.3449 - accuracy: 0.9164\n",
            "Epoch 135/200\n",
            "244/244 - 7s - loss: 0.3380 - accuracy: 0.9157\n",
            "Epoch 136/200\n",
            "244/244 - 7s - loss: 0.3332 - accuracy: 0.9159\n",
            "Epoch 137/200\n",
            "244/244 - 7s - loss: 0.3333 - accuracy: 0.9144\n",
            "Epoch 138/200\n",
            "244/244 - 7s - loss: 0.3510 - accuracy: 0.9120\n",
            "Epoch 139/200\n",
            "244/244 - 7s - loss: 0.3505 - accuracy: 0.9149\n",
            "Epoch 140/200\n",
            "244/244 - 7s - loss: 0.3263 - accuracy: 0.9148\n",
            "Epoch 141/200\n",
            "244/244 - 7s - loss: 0.3167 - accuracy: 0.9163\n",
            "Epoch 142/200\n",
            "244/244 - 7s - loss: 0.3147 - accuracy: 0.9159\n",
            "Epoch 143/200\n",
            "244/244 - 7s - loss: 0.3123 - accuracy: 0.9167\n",
            "Epoch 144/200\n",
            "244/244 - 7s - loss: 0.3101 - accuracy: 0.9159\n",
            "Epoch 145/200\n",
            "244/244 - 7s - loss: 0.3085 - accuracy: 0.9166\n",
            "Epoch 146/200\n",
            "244/244 - 7s - loss: 0.3075 - accuracy: 0.9159\n",
            "Epoch 147/200\n",
            "244/244 - 7s - loss: 0.3038 - accuracy: 0.9185\n",
            "Epoch 148/200\n",
            "244/244 - 7s - loss: 0.3061 - accuracy: 0.9155\n",
            "Epoch 149/200\n",
            "244/244 - 7s - loss: 0.3022 - accuracy: 0.9170\n",
            "Epoch 150/200\n",
            "244/244 - 7s - loss: 0.2985 - accuracy: 0.9172\n",
            "Epoch 151/200\n",
            "244/244 - 7s - loss: 0.2970 - accuracy: 0.9170\n",
            "Epoch 152/200\n",
            "244/244 - 7s - loss: 0.2957 - accuracy: 0.9163\n",
            "Epoch 153/200\n",
            "244/244 - 7s - loss: 0.2947 - accuracy: 0.9173\n",
            "Epoch 154/200\n",
            "244/244 - 7s - loss: 0.2931 - accuracy: 0.9157\n",
            "Epoch 155/200\n",
            "244/244 - 7s - loss: 0.2927 - accuracy: 0.9162\n",
            "Epoch 156/200\n",
            "244/244 - 7s - loss: 0.2910 - accuracy: 0.9173\n",
            "Epoch 157/200\n",
            "244/244 - 7s - loss: 0.2905 - accuracy: 0.9170\n",
            "Epoch 158/200\n",
            "244/244 - 7s - loss: 0.2942 - accuracy: 0.9153\n",
            "Epoch 159/200\n",
            "244/244 - 7s - loss: 0.2891 - accuracy: 0.9153\n",
            "Epoch 160/200\n",
            "244/244 - 7s - loss: 0.2896 - accuracy: 0.9163\n",
            "Epoch 161/200\n",
            "244/244 - 7s - loss: 0.2856 - accuracy: 0.9176\n",
            "Epoch 162/200\n",
            "244/244 - 7s - loss: 0.2844 - accuracy: 0.9166\n",
            "Epoch 163/200\n",
            "244/244 - 7s - loss: 0.2817 - accuracy: 0.9154\n",
            "Epoch 164/200\n",
            "244/244 - 7s - loss: 0.2803 - accuracy: 0.9159\n",
            "Epoch 165/200\n",
            "244/244 - 7s - loss: 0.2788 - accuracy: 0.9172\n",
            "Epoch 166/200\n",
            "244/244 - 7s - loss: 0.2789 - accuracy: 0.9172\n",
            "Epoch 167/200\n",
            "244/244 - 7s - loss: 0.3073 - accuracy: 0.9113\n",
            "Epoch 168/200\n",
            "244/244 - 7s - loss: 0.3348 - accuracy: 0.9057\n",
            "Epoch 169/200\n",
            "244/244 - 7s - loss: 0.2890 - accuracy: 0.9157\n",
            "Epoch 170/200\n",
            "244/244 - 7s - loss: 0.2770 - accuracy: 0.9159\n",
            "Epoch 171/200\n",
            "244/244 - 7s - loss: 0.2735 - accuracy: 0.9175\n",
            "Epoch 172/200\n",
            "244/244 - 7s - loss: 0.2726 - accuracy: 0.9152\n",
            "Epoch 173/200\n",
            "244/244 - 7s - loss: 0.2719 - accuracy: 0.9149\n",
            "Epoch 174/200\n",
            "244/244 - 7s - loss: 0.2714 - accuracy: 0.9171\n",
            "Epoch 175/200\n",
            "244/244 - 7s - loss: 0.2710 - accuracy: 0.9154\n",
            "Epoch 176/200\n",
            "244/244 - 7s - loss: 0.2710 - accuracy: 0.9175\n",
            "Epoch 177/200\n",
            "244/244 - 7s - loss: 0.2694 - accuracy: 0.9167\n",
            "Epoch 178/200\n",
            "244/244 - 7s - loss: 0.2709 - accuracy: 0.9150\n",
            "Epoch 179/200\n",
            "244/244 - 7s - loss: 0.2705 - accuracy: 0.9171\n",
            "Epoch 180/200\n",
            "244/244 - 7s - loss: 0.2693 - accuracy: 0.9164\n",
            "Epoch 181/200\n",
            "244/244 - 7s - loss: 0.2692 - accuracy: 0.9163\n",
            "Epoch 182/200\n",
            "244/244 - 7s - loss: 0.2698 - accuracy: 0.9161\n",
            "Epoch 183/200\n",
            "244/244 - 7s - loss: 0.2684 - accuracy: 0.9162\n",
            "Epoch 184/200\n",
            "244/244 - 7s - loss: 0.2678 - accuracy: 0.9168\n",
            "Epoch 185/200\n",
            "244/244 - 7s - loss: 0.2677 - accuracy: 0.9162\n",
            "Epoch 186/200\n",
            "244/244 - 7s - loss: 0.2680 - accuracy: 0.9146\n",
            "Epoch 187/200\n",
            "244/244 - 7s - loss: 0.2672 - accuracy: 0.9162\n",
            "Epoch 188/200\n",
            "244/244 - 7s - loss: 0.2702 - accuracy: 0.9171\n",
            "Epoch 189/200\n",
            "244/244 - 7s - loss: 0.2706 - accuracy: 0.9166\n",
            "Epoch 190/200\n",
            "244/244 - 7s - loss: 0.2710 - accuracy: 0.9155\n",
            "Epoch 191/200\n",
            "244/244 - 7s - loss: 0.2689 - accuracy: 0.9162\n",
            "Epoch 192/200\n",
            "244/244 - 7s - loss: 0.2666 - accuracy: 0.9167\n",
            "Epoch 193/200\n",
            "244/244 - 7s - loss: 0.2651 - accuracy: 0.9171\n",
            "Epoch 194/200\n",
            "244/244 - 7s - loss: 0.2650 - accuracy: 0.9164\n",
            "Epoch 195/200\n",
            "244/244 - 7s - loss: 0.2644 - accuracy: 0.9176\n",
            "Epoch 196/200\n",
            "244/244 - 7s - loss: 0.2647 - accuracy: 0.9143\n",
            "Epoch 197/200\n",
            "244/244 - 7s - loss: 0.2642 - accuracy: 0.9162\n",
            "Epoch 198/200\n",
            "244/244 - 7s - loss: 0.2634 - accuracy: 0.9148\n",
            "Epoch 199/200\n",
            "244/244 - 7s - loss: 0.2641 - accuracy: 0.9164\n",
            "Epoch 200/200\n",
            "244/244 - 7s - loss: 0.3028 - accuracy: 0.9079\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fde99ea2350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr1_abDn8niP"
      },
      "source": [
        "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
        "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
        "    sentence = ''\n",
        "    for _ in range(n): # n번 반복\n",
        "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
        "        encoded = pad_sequences([encoded], maxlen=23, padding='pre') # 데이터에 대한 패딩\n",
        "        result = np.argmax(model.predict(encoded), axis=-1)\n",
        "    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
        "        for word, index in t.word_index.items(): \n",
        "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
        "                break # 해당 단어가 예측 단어이므로 break\n",
        "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
        "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
        "    # for문이므로 이 행동을 다시 반복\n",
        "    sentence = init_word + sentence\n",
        "    return sentence"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dymUAHJh83Fv",
        "outputId": "8c6c7385-84fd-4e19-f4ac-ff18f812cbce"
      },
      "source": [
        "print(sentence_generation(model, t, 'i', 10))\n",
        "# 임의의 단어 'i'에 대해서 10개의 단어를 추가 생성"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i cant jump ship from facebook yet the battle of house\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcKGYZ4787bi",
        "outputId": "e13664be-058d-4949-8155-1032f4e58c8a"
      },
      "source": [
        "print(sentence_generation(model, t, 'how', 10))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "how to serve a deranged tyrant stoically home the power of\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cj8QvVaoB_Lj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}